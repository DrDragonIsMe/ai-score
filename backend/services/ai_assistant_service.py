#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIæ™ºèƒ½å­¦ä¹ ç³»ç»Ÿ - ä¸šåŠ¡æœåŠ¡ - ai_assistant_service.py

Description:
    AIåŠ©ç†"é«˜å°åˆ†"æœåŠ¡ï¼Œæä¾›æ™ºèƒ½å¯¹è¯ã€æ‹ç…§è¯†åˆ«è¯•é¢˜ã€ä¸ªæ€§åŒ–å­¦ä¹ å»ºè®®ç­‰åŠŸèƒ½ã€‚

Author: Chang Xinglong
Date: 2025-08-31
Version: 1.0.0
License: Apache License 2.0
"""

import json
import base64
import requests
from datetime import datetime
from typing import Dict, List, Any, Optional
from PIL import Image
import io
import pytesseract
from utils.database import db
from models.user import User
from models.diagnosis import DiagnosisReport, WeaknessPoint
from models.knowledge import KnowledgePoint, Subject
from models.exam import ExamSession, ExamAnalytics
from models.exam_papers import ExamPaper, KnowledgeGraph
from models.mistake import MistakeRecord, MistakeReviewSession, MistakePattern
from models.learning import StudyRecord, MemoryCard
from services.llm_service import llm_service
from services.diagnosis_service import DiagnosisService
from services.document_service import get_document_service
from services.vector_database_service import vector_db_service
from services.ppt_generation_service import ppt_generation_service
from services.knowledge_graph_service import knowledge_graph_service
from services.mastery_classification_service import mastery_classification_service
from services.pep_high_school_prompt_service import pep_prompt_service
from services.enhanced_prompt_service import enhanced_prompt_service
from utils.logger import get_logger

logger = get_logger(__name__)

class AIAssistantService:
    """
    AIåŠ©ç†"é«˜å°åˆ†"æœåŠ¡ç±»
    """
    
    def __init__(self):
        self.assistant_name = "é«˜å°åˆ†"
        self.assistant_personality = {
            "role": "æ™ºèƒ½å­¦ä¹ åŠ©æ‰‹",
            "traits": ["å‹å–„", "è€å¿ƒ", "ä¸“ä¸š", "é¼“åŠ±æ€§"],
            "greeting": "æˆ‘æ˜¯é«˜å°åˆ†ï¼Œä½ çš„ä¸“å±å­¦ä¹ åŠ©æ‰‹ã€‚æˆ‘å¯ä»¥å¸®ä½ åˆ†æè¯•é¢˜ã€åˆ¶å®šå­¦ä¹ è®¡åˆ’ï¼Œè¿˜èƒ½é€šè¿‡æ‹ç…§è¯†åˆ«é¢˜ç›®ã€‚"
        }
    
    def get_assistant_info(self) -> Dict[str, Any]:
        """
        è·å–AIåŠ©ç†åŸºæœ¬ä¿¡æ¯
        """
        return {
            "name": self.assistant_name,
            "personality": self.assistant_personality,
            "capabilities": [
                "æ™ºèƒ½å¯¹è¯äº¤æµ",
                "æ‹ç…§è¯†åˆ«è¯•é¢˜",
                "è¯•é¢˜åˆ†æè§£ç­”",
                "ä¸ªæ€§åŒ–å­¦ä¹ å»ºè®®",
                "å­¦ä¹ è¿›åº¦è·Ÿè¸ª",
                "çŸ¥è¯†ç‚¹æ¨è",
                "PDFæ–‡æ¡£åˆ†æ",
                "æ–‡æ¡£å†…å®¹é—®ç­”",
                "çº¢é»„ç»¿æŒæ¡åº¦åˆ†æ",
                "çŸ¥è¯†å›¾è°±ç”Ÿæˆ",
                "æŒ‰æ ‡ç­¾æ™ºèƒ½æ£€ç´¢"
            ]
        }
    
    def chat_with_user(self, user_id: str, message: str, context: Optional[Dict] = None, 
                      model_id: Optional[str] = None, template_id: Optional[str] = None) -> Dict[str, Any]:
        """
        ä¸ç”¨æˆ·è¿›è¡Œæ™ºèƒ½å¯¹è¯
        
        Args:
            user_id: ç”¨æˆ·ID
            message: ç”¨æˆ·æ¶ˆæ¯
            context: å¯¹è¯ä¸Šä¸‹æ–‡
            model_id: æŒ‡å®šä½¿ç”¨çš„AIæ¨¡å‹ID
            template_id: PPTæ¨¡æ¿IDï¼ˆç”¨äºPPTç”Ÿæˆï¼‰
        
        Returns:
            AIåŠ©ç†å›å¤
        """
        try:
            # æ£€æµ‹æ˜¯å¦éœ€è¦ç”ŸæˆPPT
            if self._should_generate_ppt(message):
                return self._handle_ppt_generation(user_id, message, template_id)
            
            # è·å–ç”¨æˆ·ä¿¡æ¯å’Œå­¦ä¹ æƒ…å†µ
            user_profile = self._get_user_learning_profile(user_id)
            
            # æ£€ç´¢ç»¼åˆæ•°æ®ï¼ˆæ–‡æ¡£ã€è¯•å·ã€é”™é¢˜è®°å½•ç­‰ï¼‰
            comprehensive_data = self._retrieve_comprehensive_data(user_id, message)
            
            # æ„å»ºå¯¹è¯æç¤ºè¯ï¼ˆåŒ…å«ç»¼åˆæ•°æ®ä¿¡æ¯ï¼‰
            system_prompt = self._build_comprehensive_chat_prompt(user_profile, context, comprehensive_data)
            
            # è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå›å¤
            full_prompt = f"{system_prompt}\n\nç”¨æˆ·: {message}\n\né«˜å°åˆ†:"
            
            response = llm_service.generate_text(
                prompt=full_prompt,
                max_tokens=500,
                temperature=0.7,
                model_name=model_id
            )
            
            return {
                "success": True,
                "response": response,
                "assistant_name": self.assistant_name,
                "model_used": model_id or "default",
                "timestamp": datetime.now().isoformat(),
                "suggestions": self._generate_contextual_suggestions(user_id, message),
                "referenced_data": comprehensive_data
            }
            
        except Exception as e:
            logger.error(f"AIåŠ©ç†å¯¹è¯å¤±è´¥: {str(e)}")
            return {
                "success": False,
                "response": "æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨æœ‰ç‚¹å¿™ï¼Œè¯·ç¨åå†è¯•ã€‚",
                "error": str(e)
            }
    
    def recognize_question_from_image(self, user_id: str, image_data: str) -> Dict[str, Any]:
        """
        ä»å›¾ç‰‡ä¸­è¯†åˆ«è¯•é¢˜æˆ–æè¿°å›¾ç‰‡å†…å®¹
        
        Args:
            user_id: ç”¨æˆ·ID
            image_data: Base64ç¼–ç çš„å›¾ç‰‡æ•°æ®
        
        Returns:
            è¯†åˆ«ç»“æœ
        """
        try:
            # è§£ç å›¾ç‰‡
            image_bytes = base64.b64decode(image_data)
            image = Image.open(io.BytesIO(image_bytes))
            
            # OCRè¯†åˆ«æ–‡å­—
            extracted_text = pytesseract.image_to_string(image, lang='chi_sim+eng')
            
            # å¦‚æœæ²¡æœ‰è¯†åˆ«åˆ°æ–‡å­—ï¼Œå°è¯•æè¿°å›¾ç‰‡å†…å®¹
            if not extracted_text.strip():
                image_description = self._describe_image_content(image_data, user_id)
                return {
                    "success": True,
                    "content_type": "image_description",
                    "description": image_description,
                    "message": "è¿™å¼ å›¾ç‰‡æ²¡æœ‰æ–‡å­—å†…å®¹ï¼Œæˆ‘æ¥ä¸ºä½ æè¿°ä¸€ä¸‹å›¾ç‰‡å†…å®¹ã€‚",
                    "timestamp": datetime.now().isoformat()
                }
            
            # ä½¿ç”¨AIåˆ†æè¯†åˆ«çš„æ–‡æœ¬ï¼Œåˆ¤æ–­æ˜¯å¦ä¸ºé¢˜ç›®
            content_analysis = self._analyze_extracted_text(extracted_text, user_id)
            
            logger.info(f"å›¾ç‰‡è¯†åˆ«æˆåŠŸ - æå–æ–‡æœ¬: {extracted_text[:100]}...")
            logger.info(f"å†…å®¹åˆ†æç»“æœ: {content_analysis}")
            
            # æ ¹æ®åˆ†æç»“æœå†³å®šå¤„ç†æ–¹å¼
            if content_analysis.get('is_question', False):
                return {
                    "success": True,
                    "content_type": "question",
                    "extracted_text": extracted_text,
                    "question_analysis": content_analysis,
                    "timestamp": datetime.now().isoformat()
                }
            else:
                # ä¸æ˜¯é¢˜ç›®ï¼Œæè¿°å›¾ç‰‡å†…å®¹
                image_description = self._describe_image_content(image_data, user_id, extracted_text)
                return {
                    "success": True,
                    "content_type": "image_description",
                    "extracted_text": extracted_text,
                    "description": image_description,
                    "message": "è¿™å¼ å›¾ç‰‡ä¸æ˜¯é¢˜ç›®ï¼Œæˆ‘æ¥ä¸ºä½ ä»‹ç»ä¸€ä¸‹å›¾ç‰‡å†…å®¹ã€‚",
                    "timestamp": datetime.now().isoformat()
                }
            
        except Exception as e:
            logger.error(f"å›¾ç‰‡è¯†åˆ«å¤±è´¥: {str(e)}")
            return {
                "success": False,
                "message": "å›¾ç‰‡è¯†åˆ«å¤±è´¥ï¼Œè¯·é‡è¯•ã€‚",
                "error": str(e)
            }
    
    def analyze_and_solve_question(self, user_id: str, question_text: str, 
                                 user_answer: Optional[str] = None) -> Dict[str, Any]:
        """
        åˆ†æå¹¶è§£ç­”è¯•é¢˜
        
        Args:
            user_id: ç”¨æˆ·ID
            question_text: é¢˜ç›®æ–‡æœ¬
            user_answer: ç”¨æˆ·ç­”æ¡ˆï¼ˆå¯é€‰ï¼‰
        
        Returns:
            åˆ†æç»“æœ
        """
        try:
            # è·å–ç”¨æˆ·å­¦ä¹ æƒ…å†µ
            user_profile = self._get_user_learning_profile(user_id)
            
            # æ„å»ºåˆ†ææç¤ºè¯
            analysis_prompt = self._build_question_analysis_prompt(
                question_text, user_answer, user_profile
            )
            
            # è°ƒç”¨å¤§æ¨¡å‹è¿›è¡Œåˆ†æ
            analysis_result = llm_service.generate_text(
                prompt=analysis_prompt,
                max_tokens=1000,
                temperature=0.3
            )
            
            # è§£æåˆ†æç»“æœ
            parsed_result = self._parse_analysis_result(analysis_result)
            
            # ç”Ÿæˆä¸ªæ€§åŒ–å»ºè®®
            personalized_suggestions = self._generate_personalized_suggestions(
                user_id, question_text, parsed_result
            )
            
            return {
                "success": True,
                "analysis": parsed_result,
                "suggestions": personalized_suggestions,
                "assistant_comment": self._generate_assistant_comment(parsed_result, user_answer),
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"é¢˜ç›®åˆ†æå¤±è´¥: {str(e)}")
            return {
                "success": False,
                "message": "é¢˜ç›®åˆ†æå¤±è´¥ï¼Œè¯·é‡è¯•ã€‚",
                "error": str(e)
            }
    
    def get_personalized_recommendations(self, user_id: str) -> Dict[str, Any]:
        """
        è·å–ä¸ªæ€§åŒ–å­¦ä¹ å»ºè®®
        
        Args:
            user_id: ç”¨æˆ·ID
        
        Returns:
            ä¸ªæ€§åŒ–å»ºè®®
        """
        try:
            # è·å–ç”¨æˆ·å­¦ä¹ è¯Šæ–­æ•°æ®
            user_profile = self._get_user_learning_profile(user_id)
            diagnosis_data = self._get_user_diagnosis_data(user_id)
            
            # ç”Ÿæˆä¸ªæ€§åŒ–å»ºè®®
            recommendations = {
                "daily_study_plan": self._generate_daily_study_plan(user_profile, diagnosis_data),
                "weak_points_focus": self._get_weak_points_recommendations(user_id),
                "practice_suggestions": self._generate_practice_suggestions(user_profile),
                "motivational_message": self._generate_motivational_message(user_profile)
            }
            
            return {
                "success": True,
                "recommendations": recommendations,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"è·å–ä¸ªæ€§åŒ–å»ºè®®å¤±è´¥: {str(e)}")
            return {
                "success": False,
                "message": "è·å–å»ºè®®å¤±è´¥ï¼Œè¯·é‡è¯•ã€‚",
                "error": str(e)
            }
    
    def analyze_document_content(self, user_id: str, document_id: str, question: Optional[str] = None) -> Dict[str, Any]:
        """
        åˆ†æPDFæ–‡æ¡£å†…å®¹å¹¶å›ç­”ç›¸å…³é—®é¢˜
        
        Args:
            user_id: ç”¨æˆ·ID
            document_id: æ–‡æ¡£ID
            question: ç”¨æˆ·é—®é¢˜ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            æ–‡æ¡£åˆ†æç»“æœ
        """
        try:
            document_service = get_document_service()
            
            # è·å–æ–‡æ¡£ä¿¡æ¯
            document_info = document_service.get_document_info(document_id)
            if not document_info:
                return {
                    "success": False,
                    "message": "æ–‡æ¡£ä¸å­˜åœ¨æˆ–å·²è¢«åˆ é™¤"
                }
            
            # è·å–æ–‡æ¡£å†…å®¹
            document_content = document_service.get_document_content(document_id)
            if not document_content:
                return {
                    "success": False,
                    "message": "æ— æ³•è·å–æ–‡æ¡£å†…å®¹"
                }
            
            # è·å–ç”¨æˆ·å­¦ä¹ æ¡£æ¡ˆ
            user_profile = self._get_user_learning_profile(user_id)
            
            # æ„å»ºåˆ†ææç¤ºè¯
            if question:
                # åŸºäºé—®é¢˜çš„æ–‡æ¡£é—®ç­”
                analysis_prompt = self._build_document_qa_prompt(
                    document_info, document_content, question, user_profile
                )
            else:
                # æ–‡æ¡£å†…å®¹æ€»ç»“åˆ†æ
                analysis_prompt = self._build_document_analysis_prompt(
                    document_info, document_content, user_profile
                )
            
            # è°ƒç”¨å¤§æ¨¡å‹è¿›è¡Œåˆ†æ
            analysis_result = llm_service.generate_text(
                prompt=analysis_prompt,
                max_tokens=1000,
                temperature=0.3
            )
            
            # ç”Ÿæˆå­¦ä¹ å»ºè®®
            learning_suggestions = self._generate_document_learning_suggestions(
                user_id, document_info, document_content
            )
            
            return {
                "success": True,
                "document_info": {
                    "title": document_info.get('title', ''),
                    "category": document_info.get('category', ''),
                    "upload_time": document_info.get('upload_time', '')
                },
                "analysis": analysis_result,
                "learning_suggestions": learning_suggestions,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"æ–‡æ¡£åˆ†æå¤±è´¥: {str(e)}")
            return {
                "success": False,
                "message": "æ–‡æ¡£åˆ†æå¤±è´¥ï¼Œè¯·é‡è¯•ã€‚",
                "error": str(e)
            }
    
    def search_documents_by_content(self, user_id: str, query: str, subject_filter: Optional[str] = None, 
                                   search_tags: bool = True, mastery_filter: Optional[str] = None) -> Dict[str, Any]:
        """
        æ ¹æ®å†…å®¹æœç´¢ç”¨æˆ·çš„PDFæ–‡æ¡£ï¼Œæ”¯æŒçº¢é»„ç»¿æŒæ¡åº¦è¿‡æ»¤
        
        Args:
            user_id: ç”¨æˆ·ID
            query: æœç´¢æŸ¥è¯¢
            subject_filter: å­¦ç§‘è¿‡æ»¤
            search_tags: æ˜¯å¦æœç´¢æ ‡ç­¾
            mastery_filter: æŒæ¡åº¦è¿‡æ»¤ ('red', 'yellow', 'green')
        
        Returns:
            æœç´¢ç»“æœ
        """
        try:
            document_service = get_document_service()
            
            # æœç´¢æ–‡æ¡£
            search_results = document_service.search_documents(
                query=query,
                user_id=user_id,
                tenant_id="default",
                search_tags=search_tags,
                subject_filter=subject_filter
            )
            
            if not search_results:
                return {
                    "success": True,
                    "results": [],
                    "message": "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œå»ºè®®ä¸Šä¼ æ›´å¤šå­¦ä¹ èµ„æ–™ã€‚",
                    "suggestions": [
                        "å°è¯•ä½¿ç”¨ä¸åŒçš„å…³é”®è¯æœç´¢",
                        "ä¸Šä¼ æ›´å¤šç›¸å…³çš„PDFæ–‡æ¡£",
                        "æ£€æŸ¥æœç´¢è¯çš„æ‹¼å†™"
                    ]
                }
            
            # ä¸ºæ¯ä¸ªç»“æœç”Ÿæˆç®€è¦è¯´æ˜å’ŒæŒæ¡åº¦åˆ†æ
            enhanced_results = []
            for doc in search_results[:5]:  # é™åˆ¶è¿”å›å‰5ä¸ªç»“æœ
                doc_summary = self._generate_document_summary(doc)
                
                # åˆ†ææ–‡æ¡£ç›¸å…³çŸ¥è¯†ç‚¹çš„æŒæ¡åº¦
                mastery_analysis = None
                if subject_filter:
                    mastery_analysis = self._analyze_document_mastery(user_id, doc, subject_filter)
                
                doc_result = {
                    "document_id": doc.get('id'),
                    "title": doc.get('title', ''),
                    "category": doc.get('category', ''),
                    "tags": doc.get('tags', []),
                    "relevance_score": doc.get('relevance_score', 0),
                    "match_type": doc.get('match_type', 'content'),
                    "summary": doc_summary,
                    "upload_time": doc.get('upload_time', '')
                }
                
                if mastery_analysis:
                    doc_result["mastery_analysis"] = mastery_analysis
                
                # æ ¹æ®æŒæ¡åº¦è¿‡æ»¤
                if mastery_filter and mastery_analysis:
                    if mastery_analysis.get('overall_level') == mastery_filter:
                        enhanced_results.append(doc_result)
                else:
                    enhanced_results.append(doc_result)
            
            return {
                "success": True,
                "results": enhanced_results,
                "total_found": len(search_results),
                "message": f"æ‰¾åˆ° {len(search_results)} ä¸ªç›¸å…³æ–‡æ¡£",
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"æ–‡æ¡£æœç´¢å¤±è´¥: {str(e)}")
            return {
                "success": False,
                "message": "æ–‡æ¡£æœç´¢å¤±è´¥ï¼Œè¯·é‡è¯•ã€‚",
                "error": str(e)
            }
    
    def _get_user_learning_profile(self, user_id: str) -> Dict[str, Any]:
        """
        è·å–ç”¨æˆ·å­¦ä¹ æ¡£æ¡ˆ
        """
        try:
            user = User.query.get(user_id)
            if not user:
                return {}
            
            # è·å–æœ€è¿‘çš„è¯Šæ–­æŠ¥å‘Š
            recent_reports = DiagnosisReport.query.filter_by(
                user_id=user_id
            ).order_by(DiagnosisReport.created_at.desc()).limit(3).all()
            
            profile = {
                "user_id": user_id,
                "username": user.username,
                "real_name": getattr(user, 'real_name', None),
                "nickname": getattr(user, 'nickname', None),
                "preferred_greeting": getattr(user, 'preferred_greeting', 'casual'),
                # ä¼˜å…ˆä»User.gradeè¯»å–å¹´çº§æ°´å¹³ï¼Œå¦‚æœæ²¡æœ‰åˆ™å›é€€åˆ°å¯èƒ½å­˜åœ¨çš„grade_levelå­—æ®µ
                "grade_level": (getattr(user, 'grade', None) or getattr(user, 'grade_level', 'æœªçŸ¥')),
                # å°†ä¸ªäººæè¿°å†™å…¥æ¡£æ¡ˆï¼Œåç»­æ³¨å…¥æç¤ºè¯
                "bio": getattr(user, 'bio', ''),
                "recent_reports": len(recent_reports),
                "learning_style": self._infer_learning_style(recent_reports),
                "strong_subjects": self._get_strong_subjects(recent_reports),
                "weak_areas": self._get_weak_areas(user_id)
            }
            
            return profile
            
        except Exception as e:
            logger.error(f"è·å–ç”¨æˆ·å­¦ä¹ æ¡£æ¡ˆå¤±è´¥: {str(e)}")
            return {}
    
    def _retrieve_relevant_documents(self, user_id: str, query: str) -> List[Dict[str, Any]]:
        """
        æ£€ç´¢ä¸æŸ¥è¯¢ç›¸å…³çš„æ–‡æ¡£
        
        Args:
            user_id: ç”¨æˆ·ID
            query: æŸ¥è¯¢æ–‡æœ¬
        
        Returns:
            ç›¸å…³æ–‡æ¡£åˆ—è¡¨
        """
        try:
            # ä½¿ç”¨å‘é‡æ•°æ®åº“è¿›è¡Œè¯­ä¹‰æœç´¢
            similar_docs = vector_db_service.search_similar_documents(
                query=query,
                top_k=3,
                similarity_threshold=0.4
            )
            
            if not similar_docs:
                return []
            
            # è·å–æ–‡æ¡£è¯¦ç»†ä¿¡æ¯
            document_service = get_document_service()
            relevant_documents = []
            
            for doc in similar_docs:
                try:
                    # æ ¹æ®æ–‡æ¡£ç±»å‹è·å–è¯¦ç»†ä¿¡æ¯
                    if doc['document_type'] == 'document':
                        doc_info = document_service.get_document_info(doc['document_id'])
                        if doc_info:
                            relevant_documents.append({
                                'id': doc['document_id'],
                                'type': 'document',
                                'title': doc_info.get('title', 'æœªçŸ¥æ–‡æ¡£'),
                                'content_snippet': doc['content'][:200] + '...' if len(doc['content']) > 200 else doc['content'],
                                'similarity': doc['similarity'],
                                'preview_url': f"/api/document/{doc['document_id']}/preview",
                                'metadata': doc.get('metadata', {})
                            })
                    elif doc['document_type'] == 'exam_paper':
                        # å¤„ç†è¯•å·æ–‡æ¡£
                        relevant_documents.append({
                            'id': doc['document_id'],
                            'type': 'exam_paper',
                            'title': f"è¯•å· {doc['document_id']}",
                            'content_snippet': doc['content'][:200] + '...' if len(doc['content']) > 200 else doc['content'],
                            'similarity': doc['similarity'],
                            'preview_url': f"/api/exam_papers/{doc['document_id']}/preview",
                            'metadata': doc.get('metadata', {})
                        })
                except Exception as e:
                    logger.warning(f"è·å–æ–‡æ¡£ {doc['document_id']} è¯¦ç»†ä¿¡æ¯å¤±è´¥: {str(e)}")
                    continue
            
            logger.info(f"ä¸ºæŸ¥è¯¢ '{query}' æ‰¾åˆ° {len(relevant_documents)} ä¸ªç›¸å…³æ–‡æ¡£")
            return relevant_documents
            
        except Exception as e:
            logger.error(f"æ–‡æ¡£æ£€ç´¢å¤±è´¥: {str(e)}")
            return []

    def _analyze_document_mastery(self, user_id: str, doc: Dict[str, Any], subject_id: str) -> Dict[str, Any]:
        """
        åˆ†ææ–‡æ¡£ç›¸å…³çŸ¥è¯†ç‚¹çš„æŒæ¡åº¦
        
        Args:
            user_id: ç”¨æˆ·ID
            doc: æ–‡æ¡£ä¿¡æ¯
            subject_id: å­¦ç§‘ID
            
        Returns:
            æŒæ¡åº¦åˆ†æç»“æœ
        """
        try:
            # ä»æ–‡æ¡£æ ‡ç­¾ä¸­æå–çŸ¥è¯†ç‚¹
            tags = doc.get('tags', [])
            knowledge_points = []
            
            # è·å–å­¦ç§‘ç›¸å…³çš„çŸ¥è¯†ç‚¹
            from models.knowledge import KnowledgePoint
            for tag in tags:
                kp = KnowledgePoint.query.filter_by(name=tag).first()
                if kp:
                    knowledge_points.append(kp.id)
            
            if not knowledge_points:
                return {
                    'overall_level': 'unknown',
                    'knowledge_points': [],
                    'red_count': 0,
                    'yellow_count': 0,
                    'green_count': 0
                }
            
            # åˆ†ææ¯ä¸ªçŸ¥è¯†ç‚¹çš„æŒæ¡åº¦
            red_count = yellow_count = green_count = 0
            kp_analysis = []
            
            for kp_id in knowledge_points:
                mastery_level = mastery_classification_service.classify_knowledge_point(user_id, kp_id)
                kp_analysis.append({
                    'knowledge_point_id': kp_id,
                    'mastery_level': mastery_level
                })
                
                if mastery_level == 'red':
                    red_count += 1
                elif mastery_level == 'yellow':
                    yellow_count += 1
                elif mastery_level == 'green':
                    green_count += 1
            
            # ç¡®å®šæ•´ä½“æŒæ¡åº¦
            total_kps = len(knowledge_points)
            if red_count > total_kps * 0.5:
                overall_level = 'red'
            elif yellow_count > total_kps * 0.3:
                overall_level = 'yellow'
            else:
                overall_level = 'green'
            
            return {
                'overall_level': overall_level,
                'knowledge_points': kp_analysis,
                'red_count': red_count,
                'yellow_count': yellow_count,
                'green_count': green_count,
                'total_count': total_kps
            }
            
        except Exception as e:
            logger.error(f"æ–‡æ¡£æŒæ¡åº¦åˆ†æå¤±è´¥: {str(e)}")
            return {
                'overall_level': 'unknown',
                'knowledge_points': [],
                'red_count': 0,
                'yellow_count': 0,
                'green_count': 0,
                'error': str(e)
            }

    def generate_knowledge_graph_for_user(self, user_id: str, subject_id: str) -> Dict[str, Any]:
        """
        ä¸ºç”¨æˆ·ç”ŸæˆçŸ¥è¯†å›¾è°±
        
        Args:
            user_id: ç”¨æˆ·ID
            subject_id: å­¦ç§‘ID
            
        Returns:
            çŸ¥è¯†å›¾è°±ç”Ÿæˆç»“æœ
        """
        try:
            # ç”ŸæˆçŸ¥è¯†å›¾è°±
            knowledge_graph = knowledge_graph_service.generate_knowledge_graph(
                subject_id=subject_id,
                user_id=user_id
            )
            
            # è·å–æŒæ¡åº¦ç»Ÿè®¡
            mastery_stats = mastery_classification_service.classify_user_knowledge_points(user_id, subject_id)
            
            return {
                'success': True,
                'knowledge_graph': knowledge_graph,
                'mastery_statistics': mastery_stats,
                'message': 'çŸ¥è¯†å›¾è°±ç”ŸæˆæˆåŠŸ',
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"çŸ¥è¯†å›¾è°±ç”Ÿæˆå¤±è´¥: {str(e)}")
            return {
                'success': False,
                'message': 'çŸ¥è¯†å›¾è°±ç”Ÿæˆå¤±è´¥ï¼Œè¯·é‡è¯•',
                'error': str(e)
            }
    
    def save_content_to_knowledge_graph(self, user_id: str, subject_id: str, content: str, tags: Optional[List[str]] = None, title: Optional[str] = None, force_overwrite: bool = False) -> Dict[str, Any]:
        """
        ä¿å­˜å†…å®¹åˆ°çŸ¥è¯†å›¾è°±
        
        Args:
            user_id: ç”¨æˆ·ID
            subject_id: å­¦ç§‘ID
            content: è¦ä¿å­˜çš„å†…å®¹
            tags: æ ‡ç­¾åˆ—è¡¨
            title: è‡ªå®šä¹‰æ ‡é¢˜ï¼Œå¦‚æœä¸æä¾›åˆ™è‡ªåŠ¨ç”Ÿæˆ
            force_overwrite: æ˜¯å¦å¼ºåˆ¶è¦†ç›–å·²å­˜åœ¨çš„åŒåå›¾è°±
            
        Returns:
            ä¿å­˜ç»“æœ
        """
        try:
            # è¿™é‡Œå¯ä»¥å°†å†…å®¹ä¿å­˜åˆ°å‘é‡æ•°æ®åº“æˆ–çŸ¥è¯†å›¾è°±å­˜å‚¨ä¸­
            # æš‚æ—¶è¿”å›æˆåŠŸå“åº”ï¼Œå®é™…å®ç°å¯ä»¥æ ¹æ®éœ€è¦æ‰©å±•
            
            # å¯ä»¥è°ƒç”¨å‘é‡æ•°æ®åº“æœåŠ¡ä¿å­˜å†…å®¹
            try:
                # ç”Ÿæˆå”¯ä¸€çš„æ–‡æ¡£ID
                document_id = f"ai_assistant_edit_{user_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                
                # å‡†å¤‡å…ƒæ•°æ®
                metadata = {
                    'user_id': user_id,
                    'subject_id': subject_id,
                    'tags': tags or [],
                    'source': 'ai_assistant_edit',
                    'timestamp': datetime.now().isoformat()
                }
                
                # å°è¯•ä¿å­˜åˆ°å‘é‡æ•°æ®åº“ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                success = vector_db_service.add_document_vectors(
                    document_id=document_id,
                    document_type='ai_assistant_content',
                    content_chunks=[content],  # å°†å†…å®¹ä½œä¸ºå•ä¸ªå—
                    metadata=metadata
                )
                
                if not success:
                    logger.warning("å‘é‡æ•°æ®åº“ä¿å­˜å¤±è´¥ï¼Œä½†å†…å®¹ä»è¢«è®°å½•")
                    
            except Exception as vector_error:
                logger.warning(f"å‘é‡æ•°æ®åº“ä¿å­˜å¤±è´¥ï¼Œä½†å†…å®¹ä»è¢«è®°å½•: {str(vector_error)}")
            
            # ä¿å­˜åˆ°çŸ¥è¯†å›¾è°±æ•°æ®åº“è¡¨
            try:
                # è·å–å­¦ç§‘ä¿¡æ¯
                subject = Subject.query.get(subject_id)
                if not subject:
                    raise ValueError(f"Subject {subject_id} not found")
                
                # ç”Ÿæˆæˆ–ä½¿ç”¨æä¾›çš„æ ‡é¢˜
                graph_name = title if title else f"AIåŠ©ç†ä¿å­˜ - {subject.name} - {datetime.now().strftime('%Y-%m-%d %H:%M')}"
                
                # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç›¸åŒåç§°çš„çŸ¥è¯†å›¾è°±
                existing_graph = KnowledgeGraph.query.filter_by(
                    subject_id=subject_id,
                    name=graph_name,
                    is_active=True
                ).first()
                
                if existing_graph and not force_overwrite:
                    # è¿”å›é‡å¤æç¤ºï¼Œè®©å‰ç«¯å¤„ç†
                    return {
                        'success': False,
                        'duplicate_found': True,
                        'existing_graph': {
                            'id': existing_graph.id,
                            'name': existing_graph.name,
                            'created_at': existing_graph.created_at.isoformat(),
                            'description': existing_graph.description
                        },
                        'message': 'å‘ç°åŒåçŸ¥è¯†å›¾è°±ï¼Œè¯·é€‰æ‹©è¦†ç›–æˆ–ä½¿ç”¨æ–°åç§°',
                        'suggested_name': f"{graph_name} - å‰¯æœ¬"
                    }
                
                # å¦‚æœéœ€è¦è¦†ç›–ï¼Œæ›´æ–°ç°æœ‰è®°å½•
                if existing_graph and force_overwrite:
                    knowledge_graph = existing_graph
                    knowledge_graph.description = f"ç”¨æˆ·é€šè¿‡AIåŠ©ç†ä¿å­˜çš„çŸ¥è¯†å†…å®¹ï¼ˆå·²æ›´æ–°ï¼‰"
                    knowledge_graph.updated_at = datetime.now()
                else:
                    # åˆ›å»ºæ–°çš„çŸ¥è¯†å›¾è°±è®°å½•
                    knowledge_graph = KnowledgeGraph(
                        subject_id=subject_id,
                        name=graph_name,
                        description=f"ç”¨æˆ·é€šè¿‡AIåŠ©ç†ä¿å­˜çš„çŸ¥è¯†å†…å®¹",
                        year=datetime.now().year,
                        graph_type='ai_assistant_content',
                    nodes=[
                        {
                            'id': f'content_{document_id}',
                            'name': f'AIåŠ©ç†å†…å®¹',
                            'type': 'ai_content',
                            'content': content,
                            'tags': tags or [],
                            'level': 1,
                            'x': 0,
                            'y': 0
                        }
                    ],
                    edges=[],
                    layout_config={
                        'layout': 'force',
                        'node_size_factor': 1.0,
                        'link_strength_factor': 1.0,
                        'color_scheme': 'default'
                    }
                )
                
                db.session.add(knowledge_graph)
                db.session.commit()
                
                logger.info(f"ç”¨æˆ· {user_id} ä¿å­˜å†…å®¹åˆ°çŸ¥è¯†å›¾è°±æˆåŠŸï¼Œå­¦ç§‘: {subject_id}ï¼Œå›¾è°±ID: {knowledge_graph.id}")
                
                return {
                    'success': True,
                    'message': 'å†…å®¹å·²æˆåŠŸä¿å­˜åˆ°çŸ¥è¯†å›¾è°±',
                    'content': content,
                    'tags': tags or [],
                    'knowledge_graph_id': knowledge_graph.id,
                    'timestamp': datetime.now().isoformat()
                }
                
            except Exception as db_error:
                logger.error(f"ä¿å­˜åˆ°çŸ¥è¯†å›¾è°±æ•°æ®åº“å¤±è´¥: {str(db_error)}")
                db.session.rollback()
                # å³ä½¿æ•°æ®åº“ä¿å­˜å¤±è´¥ï¼Œå‘é‡æ•°æ®åº“å¯èƒ½å·²ç»ä¿å­˜æˆåŠŸï¼Œæ‰€ä»¥ä»è¿”å›éƒ¨åˆ†æˆåŠŸ
                return {
                    'success': True,
                    'message': 'å†…å®¹å·²ä¿å­˜åˆ°å‘é‡æ•°æ®åº“ï¼Œä½†çŸ¥è¯†å›¾è°±è®°å½•åˆ›å»ºå¤±è´¥',
                    'content': content,
                    'tags': tags or [],
                    'timestamp': datetime.now().isoformat(),
                    'warning': 'çŸ¥è¯†å›¾è°±è®°å½•åˆ›å»ºå¤±è´¥ï¼Œä½†å†…å®¹å·²ä¿å­˜'
                }
            
        except Exception as e:
            logger.error(f"ä¿å­˜å†…å®¹åˆ°çŸ¥è¯†å›¾è°±å¤±è´¥: {str(e)}")
            return {
                'success': False,
                'message': 'ä¿å­˜åˆ°çŸ¥è¯†å›¾è°±å¤±è´¥ï¼Œè¯·é‡è¯•',
                'error': str(e)
            }

    def _retrieve_comprehensive_data(self, user_id: str, query: str) -> Dict[str, Any]:
        """
        æ£€ç´¢ç»¼åˆæ•°æ®ï¼ŒåŒ…æ‹¬æ–‡æ¡£ã€è¯•å·ã€é”™é¢˜è®°å½•ã€å­¦ä¹ æƒ…å†µå’ŒçŸ¥è¯†å›¾è°±
        
        Args:
            user_id: ç”¨æˆ·ID
            query: æŸ¥è¯¢å†…å®¹
        
        Returns:
            ç»¼åˆæ•°æ®å­—å…¸
        """
        try:
            # æ£€ç´¢ç›¸å…³æ–‡æ¡£
            documents = self._retrieve_relevant_documents(user_id, query)
            
            # æ£€ç´¢è¯•å·æ•°æ®
            exam_papers = self._get_exam_papers_data(user_id, query)
            
            # æ£€ç´¢é”™é¢˜è®°å½•
            mistake_records = self._get_mistake_records_data(user_id, query)
            
            # æ£€ç´¢è€ƒè¯•ä¼šè¯
            exam_sessions = self._get_exam_sessions_data(user_id, query)
            
            # æ£€ç´¢å­¦ä¹ è®°å½•
            study_records = self._get_study_records_data(user_id, query)
            
            # æ£€ç´¢çŸ¥è¯†å›¾è°±
            knowledge_graphs = self._get_knowledge_graphs_data(user_id, query)
            
            # è·å–å­¦ä¹ åˆ†ææ•°æ®
            learning_analytics = self._get_learning_analytics_data(user_id)
            
            return {
                'documents': documents,
                'exam_papers': exam_papers,
                'mistake_records': mistake_records,
                'exam_sessions': exam_sessions,
                'study_records': study_records,
                'knowledge_graphs': knowledge_graphs,
                'learning_analytics': learning_analytics
            }
            
        except Exception as e:
            logger.error(f"æ£€ç´¢ç»¼åˆæ•°æ®å¤±è´¥: {str(e)}")
            return {}
    
    def _build_chat_prompt(self, user_profile: Dict, context: Optional[Dict] = None, 
                          relevant_documents: Optional[List[Dict]] = None) -> str:
        """
        æ„å»ºå¯¹è¯æç¤ºè¯ï¼ˆé›†æˆäººæ•™ç‰ˆé«˜ä¸­ä¸“ä¸šæç¤ºè¯ï¼‰
        """
        # ç¡®å®šç”¨æˆ·ç§°å‘¼
        user_nickname = user_profile.get('nickname') or user_profile.get('real_name') or user_profile.get('username', 'åŒå­¦')
        preferred_greeting = user_profile.get('preferred_greeting', 'casual')
        grade_level = user_profile.get('grade_level', 'é«˜ä¸­')
        
        # æ ¹æ®é—®å€™åå¥½è®¾ç½®ç§°å‘¼æ–¹å¼
        greeting_styles = {
            'formal': f'æ‚¨å¥½ï¼Œ{user_nickname}',
            'casual': f'å—¨ï¼Œ{user_nickname}',
            'friendly': f'{user_nickname}ï¼Œä½ å¥½',
            'professional': f'{user_nickname}åŒå­¦'
        }
        
        greeting_style = greeting_styles.get(preferred_greeting, f'ä½ å¥½ï¼Œ{user_nickname}')
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å­¦ç§‘ç›¸å…³çš„å¯¹è¯å†…å®¹
        message_content = context.get('message', '') if isinstance(context, dict) else (context if isinstance(context, str) else '')
        detected_subject = self._detect_question_subject(message_content) if message_content else None
        question_type = self._detect_question_type(message_content) if message_content else 'å­¦ä¹ å’¨è¯¢'
        
        # æ„å»ºäººæ•™ç‰ˆä¸“ä¸šæç¤ºè¯
        if detected_subject and grade_level in ['é«˜ä¸€', 'é«˜äºŒ', 'é«˜ä¸‰', 'é«˜ä¸­']:
            # ä½¿ç”¨äººæ•™ç‰ˆé«˜ä¸­ä¸“ä¸šæç¤ºè¯
            context_info = {
                'difficulty_level': user_profile.get('difficulty_preference', 'ä¸­ç­‰'),
                'knowledge_points': user_profile.get('strong_subjects', []),
                'exam_type': 'å­¦ä¹ å’¨è¯¢'
            }
            
            pep_prompt = pep_prompt_service.build_subject_specific_prompt(
                subject_name=detected_subject,
                question_type=question_type,
                grade_level=grade_level,
                context=context_info
            )
            
            base_prompt = f"""{pep_prompt}

å½“å‰å¯¹è¯ä»»åŠ¡ï¼š
ç”¨æˆ·ä¿¡æ¯ï¼š
- ç§°å‘¼ï¼š{user_nickname}
- é—®å€™æ–¹å¼ï¼š{greeting_style}
- å¹´çº§ï¼š{grade_level}
- æ“…é•¿ç§‘ç›®ï¼š{', '.join(user_profile.get('strong_subjects', []))}
- è–„å¼±ç¯èŠ‚ï¼š{', '.join(user_profile.get('weak_areas', []))}

å›ç­”æ ‡å‡†ï¼š
- ç›´æ¥å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œé¿å…å¤šä½™çš„é—®å€™è¯­
- ä¸¥æ ¼æŒ‰ç…§äººæ•™ç‰ˆæ•™ææ ‡å‡†å’Œè¯¾ç¨‹è¦æ±‚
- ä½¿ç”¨è§„èŒƒçš„å­¦ç§‘æœ¯è¯­å’Œè¡¨è¾¾æ–¹å¼
- ä½“ç°ç›¸åº”çš„å­¦ç§‘æ ¸å¿ƒç´ å…»
- ç­”æ¡ˆå‡†ç¡®ã€ä¸“ä¸šã€æ˜“æ‡‚
- é€‚åˆ{grade_level}å­¦ç”Ÿçš„è®¤çŸ¥æ°´å¹³
"""
        else:
            # ä½¿ç”¨é€šç”¨æç¤ºè¯
            base_prompt = f"""
ä½ æ˜¯{self.assistant_name}ï¼Œä¸€ä¸ªå‹å–„ã€è€å¿ƒã€ä¸“ä¸šçš„AIå­¦ä¹ åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯å¸®åŠ©å­¦ç”Ÿæé«˜å­¦ä¹ æ•ˆæœã€‚

ç”¨æˆ·ä¿¡æ¯ï¼š
- ç§°å‘¼ï¼š{user_nickname}
- é—®å€™æ–¹å¼ï¼š{greeting_style}
- å¹´çº§ï¼š{user_profile.get('grade_level', 'æœªçŸ¥')}
- æ“…é•¿ç§‘ç›®ï¼š{', '.join(user_profile.get('strong_subjects', []))}
- è–„å¼±ç¯èŠ‚ï¼š{', '.join(user_profile.get('weak_areas', []))}

å¯¹è¯é£æ ¼è¦æ±‚ï¼š
- ç›´æ¥å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œé¿å…å¤šä½™çš„é—®å€™è¯­
- åœ¨å¯¹è¯ä¸­é€‚å½“ä½¿ç”¨ç”¨æˆ·çš„ç§°å‘¼
- æ ¹æ®ç”¨æˆ·åå¥½è°ƒæ•´è¯­è¨€é£æ ¼ï¼ˆæ­£å¼/äº²åˆ‡/å‹å¥½/ä¸“ä¸šï¼‰
- ä½¿ç”¨å‹å–„ã€é¼“åŠ±æ€§çš„è¯­è¨€
- æ ¹æ®ç”¨æˆ·çš„å­¦ä¹ æƒ…å†µæä¾›é’ˆå¯¹æ€§å»ºè®®
- é€‚å½“ä½¿ç”¨è¡¨æƒ…ç¬¦å·å¢åŠ äº²å’ŒåŠ›
- ä¿æŒä¸“ä¸šæ€§ï¼Œæä¾›å‡†ç¡®çš„å­¦ä¹ æŒ‡å¯¼
"""
        
        base_prompt += f"""

æ•°å­¦å…¬å¼æ ¼å¼è¦æ±‚ï¼š
- è¡Œé—´å…¬å¼å¿…é¡»ä½¿ç”¨ \\[ \\] åŒ…å›´ï¼Œä¾‹å¦‚ï¼š\\[ F = ma \\]
- è¡Œå†…å…¬å¼å¿…é¡»ä½¿ç”¨ \\( \\) åŒ…å›´ï¼Œä¾‹å¦‚ï¼šå…¶ä¸­ \\(F\\) æ˜¯åŠ›
- ç»å¯¹ä¸è¦ä½¿ç”¨æ–¹æ‹¬å· [] æˆ–åœ†æ‹¬å· () ä½œä¸ºæ•°å­¦å…¬å¼åˆ†éš”ç¬¦
- ä¸¥æ ¼éµå¾ªæ ‡å‡†LaTeXæ•°å­¦æ¨¡å¼è¯­æ³•
"""
        
        # æ·»åŠ ç›¸å…³æ–‡æ¡£ä¿¡æ¯
        if relevant_documents:
            base_prompt += "\n\nç›¸å…³å‚è€ƒèµ„æ–™ï¼š"
            for i, doc in enumerate(relevant_documents, 1):
                doc_type_name = "æ–‡æ¡£" if doc['type'] == 'document' else "è¯•å·"
                base_prompt += f"""
{i}. {doc_type_name}ï¼š{doc['title']}
   å†…å®¹æ‘˜è¦ï¼š{doc['content_snippet']}
   é¢„è§ˆé“¾æ¥ï¼š{doc['preview_url']}
   ç›¸å…³åº¦ï¼š{doc['similarity']:.2f}"""
            
            base_prompt += "\n\nå›ç­”æŒ‡å¯¼ï¼š"
            base_prompt += "\n- å½“å›ç­”æ¶‰åŠä¸Šè¿°å‚è€ƒèµ„æ–™æ—¶ï¼Œè¯·å¼•ç”¨ç›¸å…³å†…å®¹å¹¶æä¾›é¢„è§ˆé“¾æ¥"
            base_prompt += "\n- ä½¿ç”¨æ ¼å¼ï¼š[æ–‡æ¡£æ ‡é¢˜](é¢„è§ˆé“¾æ¥) æ¥å¼•ç”¨æ–‡æ¡£"
            base_prompt += "\n- ç»“åˆå‚è€ƒèµ„æ–™ä¸ºç”¨æˆ·æä¾›æ›´å‡†ç¡®ã€è¯¦ç»†çš„è§£ç­”"
        
        if context:
            base_prompt += f"\n\nå¯¹è¯ä¸Šä¸‹æ–‡ï¼š{json.dumps(context, ensure_ascii=False)}"
        
        return base_prompt
    
    def _build_comprehensive_chat_prompt(self, user_profile: Dict, context: Optional[Dict] = None, 
                                       comprehensive_data: Optional[Dict] = None) -> str:
        """
        æ„å»ºåŒ…å«ç»¼åˆæ•°æ®çš„å¯¹è¯æç¤ºè¯ï¼ˆä½¿ç”¨å¢å¼ºç‰ˆæç¤ºè¯æœåŠ¡æ ¹æ®å¹´çº§æ™ºèƒ½è°ƒç”¨ï¼‰
        """
        # ç¡®å®šç”¨æˆ·ç§°å‘¼å’Œå¹´çº§
        user_nickname = user_profile.get('nickname') or user_profile.get('real_name') or user_profile.get('username', 'åŒå­¦')
        preferred_greeting = user_profile.get('preferred_greeting', 'casual')
        grade_level = user_profile.get('grade_level', 'é«˜ä¸­')
        
        # è·å–ç”¨æˆ·ä¸Šä¼ çš„ææ–™ä¿¡æ¯
        user_materials = self._get_user_documents_summary(user_profile.get('user_id', ''))
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å­¦ç§‘ç›¸å…³çš„å¯¹è¯å†…å®¹
        message_content = context.get('message', '') if isinstance(context, dict) else (context if isinstance(context, str) else '')
        detected_subject = self._detect_question_subject(message_content) if message_content else None
        question_type = self._detect_question_type(message_content) if message_content else 'ç»¼åˆå’¨è¯¢'
        
        # æ„å»ºä¸Šä¸‹æ–‡å‚æ•°
        context_params = {
            'user_nickname': user_nickname,
            'preferred_greeting': preferred_greeting,
            'question_type': question_type,
            'subject': detected_subject,
            'user_materials': user_materials,
            'user_bio': user_profile.get('bio', ''),
            'difficulty_level': user_profile.get('difficulty_preference', 'ä¸­ç­‰'),
            'strong_subjects': user_profile.get('strong_subjects', []),
            'weak_areas': user_profile.get('weak_areas', []),
            'comprehensive_data': comprehensive_data
        }
        
        # ä½¿ç”¨å¢å¼ºç‰ˆæç¤ºè¯æœåŠ¡æ„å»ºå¹´çº§ç‰¹å®šæç¤ºè¯
        enhanced_prompt = enhanced_prompt_service.build_grade_specific_prompt(
            subject_name=detected_subject or 'é€šç”¨',
            grade_level=grade_level,
            context=context_params
        )
        
        # æ„å»ºåŸºç¡€æç¤ºè¯
        base_prompt = f"{enhanced_prompt}\n\nå½“å‰ç»¼åˆå­¦ä¹ ä»»åŠ¡ï¼š"
        
        if message_content:
            base_prompt += f"\nç”¨æˆ·é—®é¢˜ï¼š{message_content}"
        
        # æ·»åŠ ç»¼åˆæ•°æ®ä¿¡æ¯ï¼ˆç®€åŒ–å¤„ç†ï¼Œé‡ç‚¹çªå‡ºï¼‰
        if comprehensive_data:
            # æ–‡æ¡£èµ„æ–™
            documents = comprehensive_data.get('documents', [])
            if documents:
                base_prompt += "\n\nğŸ“š ç›¸å…³èµ„æ–™ï¼š"
                for i, doc in enumerate(documents[:2], 1):
                    try:
                        logger.debug(f"Processing document {i}: type={type(doc)}, content={doc}")
                        doc_title = doc.get('title', 'æœªå‘½åæ–‡æ¡£') if isinstance(doc, dict) else str(doc)
                        base_prompt += f"\n{i}. {doc_title}"
                    except Exception as e:
                        logger.error(f"Error processing document {i}: {e}, doc type: {type(doc)}, doc: {doc}")
                        base_prompt += f"\n{i}. æ–‡æ¡£å¤„ç†é”™è¯¯"
            
            # è¯•å·ä¿¡æ¯
            exam_papers = comprehensive_data.get('exam_papers', [])
            if exam_papers:
                base_prompt += "\n\nğŸ“ ç›¸å…³è¯•å·ï¼š"
                for i, paper in enumerate(exam_papers[:2], 1):
                    try:
                        logger.debug(f"Processing paper {i}: type={type(paper)}, content={paper}")
                        if isinstance(paper, dict):
                            paper_title = paper.get('title', 'æœªå‘½åè¯•å·')
                            paper_year = paper.get('year', '')
                            base_prompt += f"\n{i}. {paper_title} ({paper_year}å¹´)" if paper_year else f"\n{i}. {paper_title}"
                        else:
                            base_prompt += f"\n{i}. {str(paper)}"
                    except Exception as e:
                        logger.error(f"Error processing paper {i}: {e}, paper type: {type(paper)}, paper: {paper}")
                        base_prompt += f"\n{i}. è¯•å·å¤„ç†é”™è¯¯"
            
            # é”™é¢˜ç»Ÿè®¡
            mistake_records = comprehensive_data.get('mistake_records', [])
            if mistake_records:
                try:
                    logger.debug(f"Processing mistake_records: type={type(mistake_records)}, length={len(mistake_records)}")
                    for i, m in enumerate(mistake_records[:3]):
                        logger.debug(f"Mistake record {i}: type={type(m)}, content={m}")
                    resolved_count = sum(1 for m in mistake_records if isinstance(m, dict) and m.get('is_resolved'))
                    base_prompt += f"\n\nâŒ é”™é¢˜è®°å½•ï¼š{len(mistake_records)}é“ï¼Œå·²è§£å†³{resolved_count}é“"
                except Exception as e:
                    logger.error(f"Error processing mistake_records: {e}, type: {type(mistake_records)}")
                    base_prompt += f"\n\nâŒ é”™é¢˜è®°å½•å¤„ç†é”™è¯¯"
            
            # çŸ¥è¯†å›¾è°±
            knowledge_graphs = comprehensive_data.get('knowledge_graphs', [])
            if knowledge_graphs:
                base_prompt += "\n\nğŸ§  çŸ¥è¯†å›¾è°±ï¼š"
                for i, kg in enumerate(knowledge_graphs[:2], 1):
                    try:
                        logger.debug(f"Processing knowledge graph {i}: type={type(kg)}, content={kg}")
                        if isinstance(kg, dict):
                            kg_name = kg.get('name', 'æœªå‘½åå›¾è°±')
                            kg_subject = kg.get('subject_name', 'æœªçŸ¥å­¦ç§‘')
                            base_prompt += f"\n{i}. {kg_name} ({kg_subject})"
                        else:
                            base_prompt += f"\n{i}. {str(kg)}"
                    except Exception as e:
                        logger.error(f"Error processing knowledge graph {i}: {e}, kg type: {type(kg)}, kg: {kg}")
                        base_prompt += f"\n{i}. çŸ¥è¯†å›¾è°±å¤„ç†é”™è¯¯"
        
        # æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯
        if isinstance(context, dict):
            base_prompt += f"\n\nğŸ’¬ å¯¹è¯ä¸Šä¸‹æ–‡ï¼š{context.get('message', '')}"
        elif isinstance(context, str):
            base_prompt += f"\n\nğŸ’¬ å¯¹è¯ä¸Šä¸‹æ–‡ï¼š{context}"
        
        return base_prompt
    
    def _analyze_extracted_text(self, text: str, user_id: str) -> Dict[str, Any]:
        """
        åˆ†æOCRæå–çš„æ–‡æœ¬
        """
        prompt = f"""
è¯·åˆ†æä»¥ä¸‹ä»å›¾ç‰‡ä¸­æå–çš„æ–‡æœ¬ï¼Œåˆ¤æ–­æ˜¯å¦ä¸ºè¯•é¢˜ï¼Œå¹¶æå–å…³é”®ä¿¡æ¯ï¼š

æå–çš„æ–‡æœ¬ï¼š
{text}

è¯·ä»¥JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼ŒåŒ…å«ï¼š
1. is_question: æ˜¯å¦ä¸ºè¯•é¢˜ï¼ˆtrue/falseï¼‰
2. subject: ç§‘ç›®ï¼ˆå¦‚ï¼šæ•°å­¦ã€ç‰©ç†ã€åŒ–å­¦ç­‰ï¼‰
3. question_type: é¢˜å‹ï¼ˆå¦‚ï¼šé€‰æ‹©é¢˜ã€å¡«ç©ºé¢˜ã€è§£ç­”é¢˜ç­‰ï¼‰
4. difficulty_level: éš¾åº¦ç­‰çº§ï¼ˆ1-5ï¼‰
5. key_concepts: æ¶‰åŠçš„å…³é”®æ¦‚å¿µ
6. cleaned_question: æ¸…ç†åçš„é¢˜ç›®æ–‡æœ¬
"""
        
        try:
            result = llm_service.generate_text(prompt, max_tokens=500)
            return json.loads(result)
        except:
            return {
                "is_question": True,
                "subject": "æœªçŸ¥",
                "question_type": "æœªçŸ¥",
                "difficulty_level": 3,
                "key_concepts": [],
                "cleaned_question": text
            }
    
    def _describe_image_content(self, image_data: str, user_id: str, extracted_text: Optional[str] = None) -> str:
        """
        æè¿°å›¾ç‰‡å†…å®¹
        
        Args:
            image_data: Base64ç¼–ç çš„å›¾ç‰‡æ•°æ®
            user_id: ç”¨æˆ·ID
            extracted_text: OCRæå–çš„æ–‡æœ¬ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            å›¾ç‰‡å†…å®¹æè¿°
        """
        try:
            # æ„å»ºæè¿°æç¤º
            if extracted_text:
                prompt = f"""
è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ï¼š

å›¾ç‰‡ä¸­çš„æ–‡å­—å†…å®¹ï¼š
{extracted_text}

è¯·ç”¨å‹å¥½ã€è¯¦ç»†çš„è¯­è¨€æè¿°è¿™å¼ å›¾ç‰‡ï¼ŒåŒ…æ‹¬ï¼š
1. å›¾ç‰‡çš„ä¸»è¦å†…å®¹å’Œä¸»é¢˜
2. å›¾ç‰‡ä¸­çš„æ–‡å­—ä¿¡æ¯
3. å¯èƒ½çš„ç”¨é€”æˆ–èƒŒæ™¯
4. å…¶ä»–å€¼å¾—æ³¨æ„çš„ç»†èŠ‚

è¯·ä»¥è‡ªç„¶ã€æ˜“æ‡‚çš„æ–¹å¼æè¿°ï¼Œå°±åƒåœ¨å’Œæœ‹å‹èŠå¤©ä¸€æ ·ã€‚
"""
            else:
                prompt = """
è¿™æ˜¯ä¸€å¼ æ²¡æœ‰æ–‡å­—å†…å®¹çš„å›¾ç‰‡ã€‚è¯·æè¿°è¿™å¼ å›¾ç‰‡å¯èƒ½åŒ…å«çš„å†…å®¹ï¼Œæ¯”å¦‚ï¼š
1. å¯èƒ½æ˜¯ä»€ä¹ˆç±»å‹çš„å›¾ç‰‡ï¼ˆç…§ç‰‡ã€å›¾è¡¨ã€æ’ç”»ç­‰ï¼‰
2. å¯èƒ½çš„ä¸»é¢˜æˆ–å†…å®¹
3. å»ºè®®ç”¨æˆ·å¦‚ä½•æ›´å¥½åœ°ä½¿ç”¨å›¾ç‰‡è¯†åˆ«åŠŸèƒ½

è¯·ç”¨å‹å¥½ã€é¼“åŠ±çš„è¯­æ°”å›å¤ã€‚
"""
            
            result = llm_service.generate_text(prompt, max_tokens=300)
            return result.strip()
            
        except Exception as e:
            logger.error(f"å›¾ç‰‡å†…å®¹æè¿°å¤±è´¥: {str(e)}")
            if extracted_text:
                return f"æˆ‘çœ‹åˆ°è¿™å¼ å›¾ç‰‡åŒ…å«ä»¥ä¸‹æ–‡å­—å†…å®¹ï¼š{extracted_text[:200]}...\n\nè¿™çœ‹èµ·æ¥ä¸æ˜¯ä¸€é“é¢˜ç›®ï¼Œè€Œæ˜¯åŒ…å«æ–‡å­—ä¿¡æ¯çš„å›¾ç‰‡ã€‚å¦‚æœä½ éœ€è¦æˆ‘å¸®åŠ©åˆ†æç‰¹å®šå†…å®¹ï¼Œè¯·å‘Šè¯‰æˆ‘ä½ æƒ³äº†è§£ä»€ä¹ˆï¼"
            else:
                return "è¿™å¼ å›¾ç‰‡æ²¡æœ‰åŒ…å«æ–‡å­—å†…å®¹ã€‚å¦‚æœä½ æƒ³è®©æˆ‘è¯†åˆ«é¢˜ç›®ï¼Œè¯·ç¡®ä¿å›¾ç‰‡æ¸…æ™°ä¸”åŒ…å«é¢˜ç›®æ–‡å­—ã€‚æˆ‘ä¹Ÿå¯ä»¥å¸®ä½ åˆ†æå…¶ä»–åŒ…å«æ–‡å­—çš„å­¦ä¹ ææ–™ï¼"
    
    def _build_question_analysis_prompt(self, question: str, user_answer: Optional[str], 
                                      user_profile: Dict) -> str:
        """
        æ„å»ºé¢˜ç›®åˆ†ææç¤ºè¯ï¼ˆä½¿ç”¨å¢å¼ºç‰ˆæç¤ºè¯æœåŠ¡æ ¹æ®å¹´çº§æ™ºèƒ½è°ƒç”¨ï¼‰
        """
        # ç¡®å®šç”¨æˆ·ç§°å‘¼å’Œå¹´çº§
        user_nickname = user_profile.get('nickname') or user_profile.get('real_name') or user_profile.get('username', 'åŒå­¦')
        preferred_greeting = user_profile.get('preferred_greeting', 'casual')
        grade_level = user_profile.get('grade_level', 'é«˜ä¸­')
        
        # è·å–ç”¨æˆ·ä¸Šä¼ çš„ææ–™ä¿¡æ¯
        user_materials = self._get_user_documents_summary(user_profile.get('user_id', ''))
        
        # æ™ºèƒ½è¯†åˆ«å­¦ç§‘å’Œé¢˜ç›®ç±»å‹
        detected_subject = self._detect_question_subject(question)
        question_type = self._detect_question_type(question)
        
        # æ„å»ºä¸Šä¸‹æ–‡å‚æ•°
        context_params = {
            'user_nickname': user_nickname,
            'preferred_greeting': preferred_greeting,
            'question_type': question_type,
            'user_answer': user_answer,
            'subject': detected_subject,
            'user_materials': user_materials,
            'user_bio': user_profile.get('bio', ''),
            'difficulty_level': user_profile.get('difficulty_preference', 'ä¸­ç­‰'),
            'strong_subjects': user_profile.get('strong_subjects', []),
            'weak_areas': user_profile.get('weak_areas', [])
        }
        
        # ä½¿ç”¨å¢å¼ºç‰ˆæç¤ºè¯æœåŠ¡æ„å»ºå¹´çº§ç‰¹å®šæç¤ºè¯
        enhanced_prompt = enhanced_prompt_service.build_grade_specific_prompt(
            subject_name=detected_subject or 'é€šç”¨',
            grade_level=grade_level,
            context=context_params
        )
        
        # æå–é¢˜ç›®ä¸»é¢˜ç”¨äºæœç´¢
        topic = self._extract_topic_from_question(question)
        context_params['topic'] = topic
        
        # ä½¿ç”¨å¢å¼ºç‰ˆæç¤ºè¯æœåŠ¡æ„å»ºé›†æˆæœç´¢åŠŸèƒ½çš„æç¤ºè¯
        enhanced_prompt = enhanced_prompt_service.build_enhanced_prompt_with_search(
            subject_name=detected_subject or 'é€šç”¨',
            grade_level=grade_level,
            question_type='é¢˜ç›®åˆ†æ',
            context=context_params
        )
        
        # æ„å»ºå®Œæ•´æç¤ºè¯
        prompt = f"{enhanced_prompt}\n\né¢˜ç›®å†…å®¹ï¼š\n{question}"
        
        if user_answer:
            prompt += f"\n\nå­¦ç”Ÿç­”æ¡ˆï¼š\n{user_answer}"
        
        # æ·»åŠ çŸ¥è¯†ç‚¹è¯†åˆ«å’Œæ‹“å±•è¦æ±‚
        prompt += f"""

ç‰¹åˆ«è¦æ±‚ï¼š
1. è¯†åˆ«é¢˜ç›®æ¶‰åŠçš„æ ¸å¿ƒçŸ¥è¯†ç‚¹ï¼Œå¹¶ä¸{grade_level}æ•™æå†…å®¹ç²¾å‡†å¯¹åº”
2. å¦‚æœç”¨æˆ·æœ‰ç›¸å…³ä¸Šä¼ ææ–™ï¼Œä¼˜å…ˆå…³è”åˆ†æï¼Œæä¾›å…·ä½“é¡µç æˆ–ç« èŠ‚å¼•ç”¨
3. æä¾›2-3é“ç±»ä¼¼çš„æ‹“å±•ç»ƒä¹ é¢˜ç›®ï¼ˆéš¾åº¦é€’è¿›ï¼‰
4. æ¨èç›¸å…³çš„ç½‘ç»œå­¦ä¹ èµ„æºï¼ˆåŒ…å«å…·ä½“ç½‘å€æˆ–ç²¾ç¡®æœç´¢å…³é”®è¯ï¼‰
5. çŸ¥è¯†ç‚¹æ€»ç»“è¦å¹²ç»ƒç²¾è¾Ÿï¼Œé¿å…å†—ä½™è¡¨è¿°ï¼Œçªå‡ºæ ¸å¿ƒè¦ç‚¹
6. å¦‚æ— ç›¸å…³ææ–™ï¼Œä¸»åŠ¨æœç´¢å¹¶æä¾›ä¼˜è´¨è¯•é¢˜èµ„æºé“¾æ¥å’Œå†…å®¹
"""
        
        return prompt
    
    def _extract_topic_from_question(self, question: str) -> str:
        """ä»é¢˜ç›®ä¸­æå–ä¸»é¢˜å…³é”®è¯"""
        # ç®€å•çš„å…³é”®è¯æå–é€»è¾‘
        keywords = [
            'å‡½æ•°', 'æ–¹ç¨‹', 'ä¸ç­‰å¼', 'å‡ ä½•', 'æ¦‚ç‡', 'ç»Ÿè®¡', 'å¯¼æ•°', 'ç§¯åˆ†',
            'ä¸‰è§’å‡½æ•°', 'å‘é‡', 'æ•°åˆ—', 'ç«‹ä½“å‡ ä½•', 'è§£æå‡ ä½•', 'æ’åˆ—ç»„åˆ',
            'åŠ›å­¦', 'ç”µå­¦', 'å…‰å­¦', 'çƒ­å­¦', 'åŸå­ç‰©ç†', 'æ³¢åŠ¨', 'ç£åœº', 'ç”µåœº',
            'åŒ–å­¦ååº”', 'æœ‰æœºåŒ–å­¦', 'æ— æœºåŒ–å­¦', 'åŒ–å­¦å¹³è¡¡', 'ç”µåŒ–å­¦', 'åŒ–å­¦é”®',
            'ç»†èƒ', 'é—ä¼ ', 'ç”Ÿæ€', 'è¿›åŒ–', 'ç”Ÿç†', 'åˆ†å­ç”Ÿç‰©å­¦', 'å…ç–«',
            'å¤ä»£å²', 'è¿‘ä»£å²', 'ç°ä»£å²', 'æ”¿æ²»åˆ¶åº¦', 'ç»æµå‘å±•', 'æ–‡åŒ–äº¤æµ',
            'åœ°ç†ç¯å¢ƒ', 'æ°”å€™', 'åœ°å½¢', 'äººå£', 'åŸå¸‚', 'å†œä¸š', 'å·¥ä¸š',
            'æ”¿æ²»åˆ¶åº¦', 'ç»æµåˆ¶åº¦', 'æ³•å¾‹', 'å“²å­¦', 'é©¬å…‹æ€ä¸»ä¹‰', 'æ€æƒ³é“å¾·'
        ]
        
        for keyword in keywords:
            if keyword in question:
                return keyword
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç‰¹å®šå…³é”®è¯ï¼Œè¿”å›é€šç”¨ä¸»é¢˜
        return 'åŸºç¡€çŸ¥è¯†'
    
    def _get_user_documents_summary(self, user_id: str) -> str:
        """è·å–ç”¨æˆ·ä¸Šä¼ æ–‡æ¡£çš„æ‘˜è¦ä¿¡æ¯"""
        try:
            # ä½¿ç”¨ç°æœ‰çš„æ–‡æ¡£æ£€ç´¢åŠŸèƒ½è·å–ç”¨æˆ·æ–‡æ¡£
            documents = self._retrieve_relevant_documents(user_id, "å­¦ä¹ ææ–™")
            if not documents:
                return "æš‚æ— ç›¸å…³å­¦ä¹ ææ–™"
            
            summaries = []
            for doc in documents[:5]:  # é™åˆ¶æœ€å¤š5ä¸ªæ–‡æ¡£
                doc_info = f"ã€Š{doc.get('title', 'æœªå‘½åæ–‡æ¡£')}ã€‹"
                if doc.get('subject'):
                    doc_info += f"({doc['subject']})"
                summaries.append(doc_info)
            
            return f"ç”¨æˆ·å·²ä¸Šä¼ ææ–™ï¼š{', '.join(summaries)}"
        except Exception as e:
            logger.error(f"è·å–ç”¨æˆ·æ–‡æ¡£æ‘˜è¦å¤±è´¥: {e}")
            return "æš‚æ— ç›¸å…³å­¦ä¹ ææ–™"
    
    def _parse_analysis_result(self, result: str) -> Dict[str, Any]:
        """
        è§£æåˆ†æç»“æœ
        """
        # ç®€å•çš„ç»“æœè§£æï¼Œå®é™…å¯ä»¥æ›´å¤æ‚
        return {
            "solution": result,
            "confidence": 0.85,
            "key_points": [],
            "difficulty": "ä¸­ç­‰"
        }
    
    def _generate_personalized_suggestions(self, user_id: str, question: str, 
                                         analysis: Dict) -> List[str]:
        """
        ç”Ÿæˆä¸ªæ€§åŒ–å»ºè®®
        """
        suggestions = [
            "å»ºè®®å¤šç»ƒä¹ ç±»ä¼¼é¢˜å‹ï¼ŒåŠ å¼ºç†è§£",
            "å¯ä»¥å¤ä¹ ç›¸å…³çš„åŸºç¡€çŸ¥è¯†ç‚¹",
            "å°è¯•ç”¨ä¸åŒæ–¹æ³•è§£å†³åŒä¸€é—®é¢˜"
        ]
        
        # æ ¹æ®ç”¨æˆ·è–„å¼±ç¯èŠ‚æ·»åŠ é’ˆå¯¹æ€§å»ºè®®
        weak_areas = self._get_weak_areas(user_id)
        for area in weak_areas:
            suggestions.append(f"é’ˆå¯¹{area}ï¼Œå»ºè®®åŠ å¼ºä¸“é¡¹ç»ƒä¹ ")
        
        return suggestions[:5]  # é™åˆ¶å»ºè®®æ•°é‡
    
    def _generate_assistant_comment(self, analysis: Dict, user_answer: Optional[str]) -> str:
        """
        ç”ŸæˆåŠ©ç†è¯„è®ºï¼ˆé›†æˆäººæ•™ç‰ˆé«˜ä¸­ä¸“ä¸šæç¤ºè¯ï¼‰
        """
        # æ£€æµ‹é¢˜ç›®å­¦ç§‘å’Œç±»å‹
        question_text = analysis.get('question', '')
        detected_subject = self._detect_question_subject(question_text) if question_text else None
        question_type = self._detect_question_type(question_text) if question_text else 'é¢˜ç›®åˆ†æ'
        
        # å¦‚æœæ£€æµ‹åˆ°å­¦ç§‘ï¼Œä½¿ç”¨ä¸“ä¸šåŒ–çš„è¯„è®º
        if detected_subject:
            if user_answer:
                return f"è®©æˆ‘æ¥å¸®ä½ åˆ†æè¿™é“{detected_subject}{question_type}ã€‚"
            else:
                return f"æˆ‘æ¥ä¸ºä½ è¯¦ç»†è§£æè¿™é“{detected_subject}{question_type}ã€‚"
        else:
            # ä½¿ç”¨é€šç”¨è¯„è®º
            if user_answer:
                return f"{self._get_encouraging_comment()}ï¼Œè®©æˆ‘æ¥å¸®ä½ åˆ†æä¸€ä¸‹è¿™é“é¢˜ã€‚"
            else:
                return f"{self._get_encouraging_comment()}ï¼Œæˆ‘æ¥ä¸ºä½ è¯¦ç»†è§£æã€‚"
    
    def _get_encouraging_comment(self) -> str:
        """
        è·å–é¼“åŠ±æ€§è¯„è®º
        """
        comments = [
            "ä½ å¾ˆç”¨å¿ƒåœ¨æ€è€ƒ",
            "ç»§ç»­ä¿æŒè¿™ç§å­¦ä¹ æ€åº¦",
            "ä½ çš„æ€è·¯å¾ˆä¸é”™",
            "å­¦ä¹ å°±æ˜¯è¦è¿™æ ·ç§¯æä¸»åŠ¨",
            "æˆ‘ç›¸ä¿¡ä½ èƒ½æŒæ¡è¿™ä¸ªçŸ¥è¯†ç‚¹"
        ]
        import random
        return random.choice(comments)
    
    def _get_weak_areas(self, user_id: str) -> List[str]:
        """
        è·å–ç”¨æˆ·è–„å¼±é¢†åŸŸ
        """
        try:
            weak_points = WeaknessPoint.query.filter_by(
                user_id=user_id
            ).order_by(WeaknessPoint.severity.desc()).limit(5).all()
            
            return [point.knowledge_point.name for point in weak_points 
                   if point.knowledge_point]
        except:
            return []
    
    def _generate_contextual_suggestions(self, user_id: str, message: str) -> List[str]:
        """
        ç”Ÿæˆä¸Šä¸‹æ–‡ç›¸å…³å»ºè®®
        """
        suggestions = []
        
        # æ ¹æ®æ¶ˆæ¯å†…å®¹ç”Ÿæˆå»ºè®®
        if "é¢˜ç›®" in message or "é¢˜" in message:
            suggestions.append("ğŸ“¸ å¯ä»¥æ‹ç…§ä¸Šä¼ é¢˜ç›®ï¼Œæˆ‘æ¥å¸®ä½ åˆ†æ")
        
        if "ä¸ä¼š" in message or "éš¾" in message:
            suggestions.append("ğŸ’ª åˆ«æ‹…å¿ƒï¼Œæˆ‘ä»¬ä¸€æ­¥æ­¥æ¥è§£å†³")
            suggestions.append("ğŸ“š æˆ‘å¯ä»¥æ¨èä¸€äº›ç›¸å…³çš„ç»ƒä¹ ")
        
        if "è€ƒè¯•" in message:
            suggestions.append("ğŸ“‹ æˆ‘å¯ä»¥å¸®ä½ åˆ¶å®šå¤ä¹ è®¡åˆ’")
            suggestions.append("ğŸ¯ è®©æˆ‘åˆ†æä¸€ä¸‹ä½ çš„è–„å¼±ç¯èŠ‚")
        
        return suggestions
    
    def _get_user_diagnosis_data(self, user_id: str) -> Dict[str, Any]:
        """
        è·å–ç”¨æˆ·è¯Šæ–­æ•°æ®
        """
        try:
            return DiagnosisService.get_diagnosis_statistics(user_id)
        except:
            return {}
    
    def _generate_daily_study_plan(self, user_profile: Dict, diagnosis_data: Dict) -> Dict[str, Any]:
        """
        ç”Ÿæˆæ¯æ—¥å­¦ä¹ è®¡åˆ’
        """
        return {
            "morning": "å¤ä¹ æ˜¨æ—¥é”™é¢˜ï¼Œå·©å›ºè–„å¼±çŸ¥è¯†ç‚¹",
            "afternoon": "æ–°çŸ¥è¯†å­¦ä¹ å’Œç†è§£",
            "evening": "ç»ƒä¹ é¢˜ç›®ï¼Œæ£€éªŒå­¦ä¹ æ•ˆæœ",
            "duration": "å»ºè®®æ¯ä¸ªæ—¶æ®µ30-45åˆ†é’Ÿ"
        }
    
    def _get_weak_points_recommendations(self, user_id: str) -> List[Dict[str, Any]]:
        """
        è·å–è–„å¼±ç‚¹å»ºè®®
        """
        weak_areas = self._get_weak_areas(user_id)
        recommendations = []
        
        for area in weak_areas[:3]:  # åªå–å‰3ä¸ª
            recommendations.append({
                "knowledge_point": area,
                "suggestion": f"å»ºè®®åŠ å¼º{area}çš„åŸºç¡€ç»ƒä¹ ",
                "priority": "é«˜"
            })
        
        return recommendations
    
    def _generate_practice_suggestions(self, user_profile: Dict) -> List[str]:
        """
        ç”Ÿæˆç»ƒä¹ å»ºè®®
        """
        return [
            "æ¯å¤©åšæŒåš3-5é“ç›¸å…³é¢˜ç›®",
            "å®šæœŸå›é¡¾é”™é¢˜æœ¬",
            "å°è¯•ä¸åŒéš¾åº¦çš„é¢˜ç›®æŒ‘æˆ˜è‡ªå·±",
            "ä¸åŒå­¦è®¨è®ºäº¤æµå­¦ä¹ å¿ƒå¾—"
        ]
    
    def _generate_motivational_message(self, user_profile: Dict) -> str:
        """
        ç”Ÿæˆæ¿€åŠ±æ¶ˆæ¯
        """
        messages = [
            "å­¦ä¹ æ˜¯ä¸€ä¸ªæŒç»­çš„è¿‡ç¨‹ï¼Œä½ å·²ç»åœ¨è¿›æ­¥çš„è·¯ä¸Šäº†ï¼ğŸ’ª",
            "æ¯ä¸€æ¬¡ç»ƒä¹ éƒ½æ˜¯å‘ç›®æ ‡è¿ˆè¿›çš„ä¸€æ­¥ï¼ŒåŠ æ²¹ï¼ğŸŒŸ",
            "ç›¸ä¿¡è‡ªå·±ï¼Œä½ æœ‰èƒ½åŠ›å…‹æœå­¦ä¹ ä¸­çš„å›°éš¾ï¼ğŸš€",
            "ä»Šå¤©çš„åŠªåŠ›å°±æ˜¯æ˜å¤©çš„æ”¶è·ï¼Œç»§ç»­ä¿æŒï¼ğŸ“ˆ"
        ]
        import random
        return random.choice(messages)
    
    def _infer_learning_style(self, reports: List) -> str:
        """
        æ¨æ–­å­¦ä¹ é£æ ¼
        """
        if not reports:
            return "æ¢ç´¢å‹"
        
        # ç®€å•çš„å­¦ä¹ é£æ ¼æ¨æ–­é€»è¾‘
        styles = ["è§†è§‰å‹", "å¬è§‰å‹", "åŠ¨æ‰‹å‹", "é€»è¾‘å‹"]
        import random
        return random.choice(styles)
    
    def _get_strong_subjects(self, reports: List) -> List[str]:
        """
        è·å–æ“…é•¿ç§‘ç›®
        """
        # ç®€å•å®ç°ï¼Œå®é™…åº”è¯¥åŸºäºè¯Šæ–­æ•°æ®åˆ†æ
        return ["æ•°å­¦", "ç‰©ç†"] if reports else []
    
    def _build_document_qa_prompt(self, document_info: Dict, document_content: str, 
                                question: str, user_profile: Dict) -> str:
        """
        æ„å»ºæ–‡æ¡£é—®ç­”æç¤ºè¯
        """
        # ç¡®å®šç”¨æˆ·ç§°å‘¼
        user_nickname = user_profile.get('nickname') or user_profile.get('real_name') or user_profile.get('username', 'åŒå­¦')
        preferred_greeting = user_profile.get('preferred_greeting', 'casual')
        
        # æ ¹æ®é—®å€™åå¥½è®¾ç½®ç§°å‘¼æ–¹å¼
        greeting_styles = {
            'formal': f'æ‚¨å¥½ï¼Œ{user_nickname}',
            'casual': f'å—¨ï¼Œ{user_nickname}',
            'friendly': f'{user_nickname}ï¼Œä½ å¥½',
            'professional': f'{user_nickname}åŒå­¦'
        }
        
        greeting_style = greeting_styles.get(preferred_greeting, f'ä½ å¥½ï¼Œ{user_nickname}')
        
        return f"""
ä½ æ˜¯é«˜å°åˆ†ï¼Œä¸€ä¸ªä¸“ä¸šçš„å­¦ä¹ åŠ©æ‰‹ã€‚ç”¨æˆ·ä¸Šä¼ äº†ä¸€ä¸ªPDFæ–‡æ¡£ï¼Œç°åœ¨æƒ³è¦è¯¢é—®ç›¸å…³é—®é¢˜ã€‚

æ–‡æ¡£ä¿¡æ¯ï¼š
- æ ‡é¢˜ï¼š{document_info.get('title', 'æœªçŸ¥')}
- åˆ†ç±»ï¼š{document_info.get('category', 'æœªçŸ¥')}

æ–‡æ¡£å†…å®¹ï¼ˆå‰2000å­—ç¬¦ï¼‰ï¼š
{document_content[:2000]}...

ç”¨æˆ·é—®é¢˜ï¼š{question}

ç”¨æˆ·ä¿¡æ¯ï¼š
- ç§°å‘¼ï¼š{user_nickname}
- é—®å€™æ–¹å¼ï¼š{greeting_style}
- å¹´çº§ï¼š{user_profile.get('grade_level', 'æœªçŸ¥')}
- å¼ºåŠ¿å­¦ç§‘ï¼š{', '.join(user_profile.get('strong_subjects', []))}
- è–„å¼±ç¯èŠ‚ï¼š{', '.join(user_profile.get('weak_areas', []))}

è¯·åŸºäºæ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œå¹¶ç»“åˆç”¨æˆ·çš„å­¦ä¹ æƒ…å†µç»™å‡ºä¸ªæ€§åŒ–çš„å­¦ä¹ å»ºè®®ã€‚
å›ç­”è¦æ±‚ï¼š
1. ç›´æ¥å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œé¿å…å¤šä½™çš„é—®å€™è¯­
2. å¼•ç”¨æ–‡æ¡£ä¸­çš„ç›¸å…³å†…å®¹
3. æä¾›å­¦ä¹ å»ºè®®
4. æ ¹æ®ç”¨æˆ·åå¥½è°ƒæ•´è¯­è¨€é£æ ¼ï¼ˆæ­£å¼/äº²åˆ‡/å‹å¥½/ä¸“ä¸šï¼‰
5. è¯­è¨€è¦å‹å–„ã€é¼“åŠ±æ€§
"""
    
    def _build_document_analysis_prompt(self, document_info: Dict, document_content: str, 
                                      user_profile: Dict) -> str:
        """
        æ„å»ºæ–‡æ¡£åˆ†ææç¤ºè¯
        """
        # ç¡®å®šç”¨æˆ·ç§°å‘¼
        user_nickname = user_profile.get('nickname') or user_profile.get('real_name') or user_profile.get('username', 'åŒå­¦')
        preferred_greeting = user_profile.get('preferred_greeting', 'casual')
        
        # æ ¹æ®é—®å€™åå¥½è®¾ç½®ç§°å‘¼æ–¹å¼
        greeting_styles = {
            'formal': f'æ‚¨å¥½ï¼Œ{user_nickname}',
            'casual': f'å—¨ï¼Œ{user_nickname}',
            'friendly': f'{user_nickname}ï¼Œä½ å¥½',
            'professional': f'{user_nickname}åŒå­¦'
        }
        
        greeting_style = greeting_styles.get(preferred_greeting, f'ä½ å¥½ï¼Œ{user_nickname}')
        
        return f"""
ä½ æ˜¯é«˜å°åˆ†ï¼Œä¸€ä¸ªä¸“ä¸šçš„å­¦ä¹ åŠ©æ‰‹ã€‚ç”¨æˆ·ä¸Šä¼ äº†ä¸€ä¸ªPDFæ–‡æ¡£ï¼Œè¯·å¸®åŠ©åˆ†ææ–‡æ¡£å†…å®¹ã€‚

æ–‡æ¡£ä¿¡æ¯ï¼š
- æ ‡é¢˜ï¼š{document_info.get('title', 'æœªçŸ¥')}
- åˆ†ç±»ï¼š{document_info.get('category', 'æœªçŸ¥')}

æ–‡æ¡£å†…å®¹ï¼ˆå‰2000å­—ç¬¦ï¼‰ï¼š
{document_content[:2000]}...

ç”¨æˆ·ä¿¡æ¯ï¼š
- ç§°å‘¼ï¼š{user_nickname}
- é—®å€™æ–¹å¼ï¼š{greeting_style}
- å¹´çº§ï¼š{user_profile.get('grade_level', 'æœªçŸ¥')}
- å¼ºåŠ¿å­¦ç§‘ï¼š{', '.join(user_profile.get('strong_subjects', []))}
- è–„å¼±ç¯èŠ‚ï¼š{', '.join(user_profile.get('weak_areas', []))}

è¯·åˆ†æè¿™ä¸ªæ–‡æ¡£çš„å†…å®¹ï¼ŒåŒ…æ‹¬ï¼š
1. æ–‡æ¡£ä¸»è¦å†…å®¹æ¦‚è¿°
2. æ¶‰åŠçš„çŸ¥è¯†ç‚¹
3. éš¾åº¦ç­‰çº§è¯„ä¼°
4. å¯¹ç”¨æˆ·çš„å­¦ä¹ ä»·å€¼
5. ä¸ªæ€§åŒ–å­¦ä¹ å»ºè®®

å›ç­”è¦æ±‚ï¼š
- ç›´æ¥è¿›è¡Œæ–‡æ¡£åˆ†æï¼Œé¿å…å¤šä½™çš„é—®å€™è¯­
- æ ¹æ®ç”¨æˆ·åå¥½è°ƒæ•´è¯­è¨€é£æ ¼ï¼ˆæ­£å¼/äº²åˆ‡/å‹å¥½/ä¸“ä¸šï¼‰
- å›ç­”è¦å‹å–„ã€ä¸“ä¸šï¼Œå¹¶å…·æœ‰é¼“åŠ±æ€§
"""
    
    def _generate_document_learning_suggestions(self, user_id: str, document_info: Dict, 
                                             document_content: str) -> List[str]:
        """
        ç”ŸæˆåŸºäºæ–‡æ¡£çš„å­¦ä¹ å»ºè®®
        """
        suggestions = []
        
        # åŸºäºæ–‡æ¡£ç±»åˆ«çš„å»ºè®®
        category = document_info.get('category', '')
        if 'æ•°å­¦' in category:
            suggestions.extend([
                "å»ºè®®å…ˆç†è§£åŸºæœ¬æ¦‚å¿µï¼Œå†ç»ƒä¹ ç›¸å…³é¢˜ç›®",
                "å¯ä»¥åˆ¶ä½œæ€ç»´å¯¼å›¾æ•´ç†çŸ¥è¯†ç‚¹"
            ])
        elif 'è¯­æ–‡' in category:
            suggestions.extend([
                "å»ºè®®å¤šè¯»å‡ éï¼Œç†è§£æ–‡ç« ç»“æ„å’Œä¸»æ—¨",
                "å¯ä»¥æ‘˜æŠ„å¥½è¯å¥½å¥ï¼Œç§¯ç´¯å†™ä½œç´ æ"
            ])
        elif 'è‹±è¯­' in category:
            suggestions.extend([
                "å»ºè®®å…ˆæŒæ¡ç”Ÿè¯ï¼Œå†ç†è§£æ–‡ç« å†…å®¹",
                "å¯ä»¥æœ—è¯»æ–‡ç« ï¼Œæé«˜è¯­æ„Ÿ"
            ])
        
        # é€šç”¨å»ºè®®
        suggestions.extend([
            "å»ºè®®åˆ¶å®šå­¦ä¹ è®¡åˆ’ï¼Œåˆ†é˜¶æ®µæŒæ¡å†…å®¹",
            "é‡åˆ°ä¸æ‡‚çš„åœ°æ–¹å¯ä»¥éšæ—¶é—®æˆ‘",
            "å­¦ä¹ åå¯ä»¥åšç›¸å…³ç»ƒä¹ å·©å›ºçŸ¥è¯†"
        ])
        
        return suggestions[:5]  # è¿”å›å‰5ä¸ªå»ºè®®
    
    def _generate_document_summary(self, document: Dict) -> str:
        """
        ç”Ÿæˆæ–‡æ¡£æ‘˜è¦
        """
        title = document.get('title', 'æœªçŸ¥æ–‡æ¡£')
        category = document.get('category', 'æœªåˆ†ç±»')
        content_preview = document.get('content', '')[:100] + '...' if document.get('content') else 'æš‚æ— å†…å®¹é¢„è§ˆ'
        
        return f"ã€Š{title}ã€‹- {category}ç±»æ–‡æ¡£ã€‚{content_preview}"
    
    def _get_exam_papers_data(self, user_id: str, query: Optional[str] = None) -> List[Dict[str, Any]]:
        """
        è·å–è¯•å·æ•°æ®
        
        Args:
            user_id: ç”¨æˆ·ID
            query: æŸ¥è¯¢å…³é”®è¯ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            è¯•å·æ•°æ®åˆ—è¡¨
        """
        try:
            # æ„å»ºæŸ¥è¯¢
            exam_query = db.session.query(ExamPaper)
            
            if query:
                filters = []
                if hasattr(ExamPaper, 'title'):
                    filters.append(ExamPaper.title.contains(query))
                if hasattr(ExamPaper, 'description'):
                    filters.append(ExamPaper.description.contains(query))
                if hasattr(ExamPaper, 'exam_type'):
                    filters.append(ExamPaper.exam_type.contains(query))
                if hasattr(ExamPaper, 'region'):
                    filters.append(ExamPaper.region.contains(query))
                if filters:
                    exam_query = exam_query.filter(db.or_(*filters))
            
            # é™åˆ¶è¿”å›æ•°é‡
            exam_papers = exam_query.limit(10).all()
            
            papers_data = []
            for paper in exam_papers:
                papers_data.append({
                    'id': paper.id,
                    'title': paper.title,
                    'description': paper.description,
                    'year': paper.year,
                    'exam_type': paper.exam_type,
                    'region': paper.region,
                    'total_score': paper.total_score,
                    'duration': paper.duration,
                    'difficulty_level': paper.difficulty_level,
                    'question_count': paper.question_count,
                    'download_count': paper.download_count
                })
            
            return papers_data
            
        except Exception as e:
            logger.error(f"è·å–è¯•å·æ•°æ®å¤±è´¥: {str(e)}")
            return []
    
    def _get_mistake_records_data(self, user_id: str, query: Optional[str] = None) -> List[Dict[str, Any]]:
        """
        è·å–é”™é¢˜è®°å½•æ•°æ®
        
        Args:
            user_id: ç”¨æˆ·ID
            query: æŸ¥è¯¢å…³é”®è¯ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            é”™é¢˜è®°å½•æ•°æ®åˆ—è¡¨
        """
        try:
            # æ„å»ºæŸ¥è¯¢
            mistake_query = db.session.query(MistakeRecord).filter(
                MistakeRecord.user_id == user_id,
                MistakeRecord.is_archived == False
            )
            
            if query:
                # å…³è”æŸ¥è¯¢é¢˜ç›®å†…å®¹
                from models.question import Question
                mistake_query = mistake_query.join(Question).filter(
                    db.or_(
                        Question.content.contains(query),
                        MistakeRecord.error_analysis.contains(query)
                    )
                )
            
            # æŒ‰æ—¶é—´æ’åºï¼Œé™åˆ¶è¿”å›æ•°é‡
            mistakes = mistake_query.order_by(
                MistakeRecord.created_time.desc()
            ).limit(20).all()
            
            mistakes_data = []
            for mistake in mistakes:
                mistakes_data.append({
                    'id': mistake.id,
                    'question_id': mistake.question_id,
                    'user_answer': mistake.user_answer,
                    'correct_answer': mistake.correct_answer,
                    'mistake_type': mistake.mistake_type.value if hasattr(mistake, 'mistake_type') and mistake.mistake_type is not None else None,
                    'mistake_level': mistake.mistake_level.value if hasattr(mistake, 'mistake_level') and mistake.mistake_level is not None else None,
                    'error_analysis': mistake.error_analysis,
                    'solution_steps': mistake.solution_steps,
                    'key_concepts': mistake.key_concepts,
                    'improvement_suggestions': mistake.improvement_suggestions,
                    'review_count': mistake.review_count,
                    'mastery_level': mistake.mastery_level,
                    'is_resolved': mistake.is_resolved,
                    'priority_score': getattr(mistake, 'priority_score', 0),
                    'created_time': mistake.created_time.isoformat() if hasattr(mistake, 'created_time') and mistake.created_time is not None else None,
                    'next_review_time': mistake.next_review_time.isoformat() if hasattr(mistake, 'next_review_time') and mistake.next_review_time is not None and hasattr(mistake.next_review_time, 'isoformat') else None
                })
            
            return mistakes_data
            
        except Exception as e:
            logger.error(f"è·å–é”™é¢˜è®°å½•å¤±è´¥: {str(e)}")
            return []
    
    def _get_exam_sessions_data(self, user_id: str, query: Optional[str] = None) -> List[Dict[str, Any]]:
        """
        è·å–è€ƒè¯•ä¼šè¯æ•°æ®
        
        Args:
            user_id: ç”¨æˆ·ID
            query: æŸ¥è¯¢å…³é”®è¯ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            è€ƒè¯•ä¼šè¯æ•°æ®åˆ—è¡¨
        """
        try:
            # æ„å»ºæŸ¥è¯¢
            exam_query = db.session.query(ExamSession).filter(
                ExamSession.user_id == user_id
            )
            
            if query:
                filters = []
                if hasattr(ExamSession, 'title'):
                    filters.append(ExamSession.title.contains(query))
                if hasattr(ExamSession, 'description'):
                    filters.append(ExamSession.description.contains(query))
                if hasattr(ExamSession, 'exam_type'):
                    filters.append(ExamSession.exam_type.contains(query))
                if filters:
                    exam_query = exam_query.filter(db.or_(*filters))
            
            # æŒ‰æ—¶é—´æ’åºï¼Œé™åˆ¶è¿”å›æ•°é‡
            exams = exam_query.order_by(
                ExamSession.created_time.desc()
            ).limit(15).all()
            
            exams_data = []
            for exam in exams:
                exams_data.append({
                    'id': exam.id,
                    'title': exam.title,
                    'description': exam.description,
                    'exam_type': exam.exam_type,
                    'status': exam.status,
                    'total_questions': exam.total_questions,
                    'completed_questions': exam.completed_questions,
                    'total_score': exam.total_score,
                    'score_percentage': exam.score_percentage,
                    'correct_answers': exam.correct_answers,
                    'wrong_answers': exam.wrong_answers,
                    'time_efficiency': exam.time_efficiency,
                    'average_time_per_question': exam.average_time_per_question,
                    'created_time': exam.created_time.isoformat() if hasattr(exam, 'created_time') and exam.created_time is not None else None,
                    'actual_start_time': exam.actual_start_time.isoformat() if hasattr(exam, 'actual_start_time') and exam.actual_start_time is not None else None,
                    'end_time': exam.end_time.isoformat() if hasattr(exam, 'end_time') and exam.end_time is not None else None
                })
            
            return exams_data
            
        except Exception as e:
            logger.error(f"è·å–è€ƒè¯•ä¼šè¯æ•°æ®å¤±è´¥: {str(e)}")
            return []
    
    def _get_study_records_data(self, user_id: str, query: Optional[str] = None) -> List[Dict[str, Any]]:
        """
        è·å–å­¦ä¹ è®°å½•æ•°æ®
        
        Args:
            user_id: ç”¨æˆ·ID
            query: æŸ¥è¯¢å…³é”®è¯ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            å­¦ä¹ è®°å½•æ•°æ®åˆ—è¡¨
        """
        try:
            # æ„å»ºæŸ¥è¯¢
            study_query = db.session.query(StudyRecord).filter(
                StudyRecord.user_id == user_id
            )
            
            if query:
                filters = []
                if hasattr(StudyRecord, 'study_type') and hasattr(StudyRecord.study_type.property, 'columns'):
                    filters.append(StudyRecord.study_type.contains(query))
                if hasattr(StudyRecord, 'content_type') and hasattr(StudyRecord.content_type.property, 'columns'):
                    filters.append(StudyRecord.content_type.contains(query))
                if hasattr(StudyRecord, 'notes') and hasattr(StudyRecord.notes.property, 'columns'):
                    filters.append(StudyRecord.notes.contains(query))
                if filters:
                    study_query = study_query.filter(db.or_(*filters))
            
            # æŒ‰æ—¶é—´æ’åºï¼Œé™åˆ¶è¿”å›æ•°é‡
            records = study_query.order_by(
                StudyRecord.start_time.desc()
            ).limit(20).all()
            
            records_data = []
            for record in records:
                records_data.append({
                    'id': record.id,
                    'study_type': record.study_type,
                    'content_type': record.content_type,
                    'is_correct': record.is_correct,
                    'score': record.score,
                    'max_score': record.max_score,
                    'duration': record.duration,
                    'mastery_level': record.mastery_level,
                    'difficulty_rating': record.difficulty_rating,
                    'confidence_level': record.confidence_level,
                    'error_types': record.error_types,
                    'improvement_areas': record.improvement_areas,
                    'notes': record.notes,
                    'start_time': record.start_time.isoformat() if record.start_time else None,
                    'end_time': record.end_time.isoformat() if record.end_time else None
                })
            
            return records_data
            
        except Exception as e:
            logger.error(f"è·å–å­¦ä¹ è®°å½•å¤±è´¥: {str(e)}")
            return []
    
    def _get_knowledge_graphs_data(self, user_id: str, query: Optional[str] = None) -> List[Dict[str, Any]]:
        """
        è·å–ç”¨æˆ·çš„çŸ¥è¯†å›¾è°±æ•°æ®ï¼Œæ”¯æŒæŒ‰æŸ¥è¯¢å†…å®¹è¿‡æ»¤å’Œæ™ºèƒ½è¯­ä¹‰æœç´¢
        
        Args:
            user_id: ç”¨æˆ·ID
            query: æŸ¥è¯¢å†…å®¹ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            çŸ¥è¯†å›¾è°±æ•°æ®åˆ—è¡¨
        """
        try:
            # æ„å»ºåŸºç¡€æŸ¥è¯¢
            knowledge_graphs_query = KnowledgeGraph.query.filter_by(
                is_active=True
            )
            
            # å¦‚æœæœ‰æŸ¥è¯¢å†…å®¹ï¼Œè¿›è¡Œæ™ºèƒ½è¿‡æ»¤
            if query:
                query_lower = query.lower()
                
                # æå–æŸ¥è¯¢ä¸­çš„å…³é”®è¯å’Œå­¦ç§‘ä¿¡æ¯
                search_keywords = self._extract_search_keywords(query)
                subject_hints = self._extract_subject_hints(query)
                
                # æ„å»ºæœç´¢æ¡ä»¶
                search_conditions = []
                
                # åŸºç¡€æ–‡æœ¬åŒ¹é…
                for keyword in search_keywords:
                    search_conditions.extend([
                        KnowledgeGraph.name.ilike(f'%{keyword}%'),
                        KnowledgeGraph.description.ilike(f'%{keyword}%')
                    ])
                
                # æ³¨æ„ï¼šæ ‡ç­¾æœç´¢å·²ç§»è‡³åº”ç”¨å±‚å¤„ç†ï¼Œå› ä¸ºæ ‡ç­¾ç°åœ¨å­˜å‚¨åœ¨nodesæ•°ç»„ä¸­
                
                # å¦‚æœè¯†åˆ«å‡ºå­¦ç§‘ï¼Œæ·»åŠ å­¦ç§‘è¿‡æ»¤
                if subject_hints:
                    subject_ids = []
                    for subject_hint in subject_hints:
                        subjects = Subject.query.filter(
                            Subject.name.ilike(f'%{subject_hint}%')
                        ).all()
                        subject_ids.extend([s.id for s in subjects])
                    
                    if subject_ids:
                        search_conditions.append(
                            KnowledgeGraph.subject_id.in_(subject_ids)
                        )
                
                if search_conditions:
                    knowledge_graphs_query = knowledge_graphs_query.filter(
                        db.or_(*search_conditions)
                    )
            
            knowledge_graphs = knowledge_graphs_query.order_by(
                KnowledgeGraph.updated_at.desc()
            ).limit(20).all()
            
            result = []
            for kg in knowledge_graphs:
                # è·å–å­¦ç§‘ä¿¡æ¯
                subject = Subject.query.get(kg.subject_id)
                subject_name = subject.name if subject else 'æœªçŸ¥å­¦ç§‘'
                
                # è®¡ç®—æ™ºèƒ½ç›¸å…³æ€§åˆ†æ•°
                relevance_score = self._calculate_knowledge_graph_relevance(
                    kg, query, subject_name
                )
                
                # ä»nodesæ•°ç»„ä¸­æå–æ ‡ç­¾
                node_tags = []
                if kg.nodes:
                    for node in kg.nodes:
                        if isinstance(node, dict) and 'tags' in node:
                            node_tags.extend(node.get('tags', []))
                # å»é‡
                node_tags = list(set(node_tags))
                
                kg_data = {
                    'id': kg.id,
                    'name': kg.name,
                    'description': kg.description or '',
                    'subject_name': subject_name,
                    'subject_id': kg.subject_id,
                    'tags': node_tags,
                    'graph_type': kg.graph_type,
                    'created_at': kg.created_at.isoformat() if kg.created_at else '',
                    'updated_at': kg.updated_at.isoformat() if kg.updated_at else '',
                    'relevance_score': relevance_score,
                    'data_type': 'knowledge_graph'
                }
                
                result.append(kg_data)
            
            # åº”ç”¨å±‚æ ‡ç­¾æœç´¢è¿‡æ»¤
            if query:
                search_keywords = self._extract_search_keywords(query)
                if search_keywords:
                    # æ ‡ç­¾åŒ¹é…è¿‡æ»¤
                    filtered_result = []
                    for item in result:
                        item_tags = [tag.lower() for tag in item.get('tags', [])]
                        # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•æœç´¢å…³é”®è¯åŒ¹é…æ ‡ç­¾
                        tag_match = any(keyword.lower() in ' '.join(item_tags) for keyword in search_keywords)
                        if tag_match or item['relevance_score'] > 0:
                            filtered_result.append(item)
                    result = filtered_result
                
                # æŒ‰ç›¸å…³æ€§åˆ†æ•°æ’åº
                result.sort(key=lambda x: x['relevance_score'], reverse=True)
                # åªè¿”å›ç›¸å…³æ€§åˆ†æ•°å¤§äº0æˆ–æœ‰æ ‡ç­¾åŒ¹é…çš„ç»“æœ
                result = [item for item in result if isinstance(item, dict) and (item.get('relevance_score', 0) > 0 or any(keyword.lower() in ' '.join([tag.lower() for tag in item.get('tags', []) if isinstance(tag, str)]) for keyword in search_keywords))]
            
            return result[:10]  # é™åˆ¶è¿”å›æ•°é‡
            
        except Exception as e:
            logger.error(f"è·å–çŸ¥è¯†å›¾è°±æ•°æ®å¤±è´¥: {str(e)}")
            return []
    
    def _extract_search_keywords(self, query: str) -> List[str]:
        """
        ä»æŸ¥è¯¢ä¸­æå–å…³é”®è¯
        """
        import re
        
        # ç§»é™¤å¸¸è§çš„åœç”¨è¯
        stop_words = {'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æœ‰', 'å’Œ', 'ä¸', 'æˆ–', 'ä½†', 'è€Œ', 'ä¸­', 'å…³äº', 'ç»™æˆ‘', 'åˆ—å‡º', 'æ‰€æœ‰'}
        
        # åˆ†è¯å¹¶æ¸…ç†
        words = re.findall(r'[\u4e00-\u9fff]+|[a-zA-Z]+', query)
        keywords = [word.lower() for word in words if len(word) > 1 and word not in stop_words]
        
        return keywords
    
    def _extract_subject_hints(self, query: str) -> List[str]:
        """
        ä»æŸ¥è¯¢ä¸­æå–å­¦ç§‘æç¤º
        """
        subject_keywords = {
            'è¯­æ–‡': ['è¯­æ–‡', 'æ–‡å­¦', 'å¤è¯—', 'è¯—è¯', 'ä½œæ–‡', 'é˜…è¯»', 'æ–‡è¨€æ–‡'],
            'æ•°å­¦': ['æ•°å­¦', 'å‡ ä½•', 'ä»£æ•°', 'å‡½æ•°', 'æ–¹ç¨‹', 'è®¡ç®—'],
            'è‹±è¯­': ['è‹±è¯­', 'å•è¯', 'è¯­æ³•', 'é˜…è¯»ç†è§£', 'ä½œæ–‡'],
            'ç‰©ç†': ['ç‰©ç†', 'åŠ›å­¦', 'ç”µå­¦', 'å…‰å­¦', 'çƒ­å­¦'],
            'åŒ–å­¦': ['åŒ–å­¦', 'å…ƒç´ ', 'ååº”', 'åˆ†å­', 'åŸå­'],
            'ç”Ÿç‰©': ['ç”Ÿç‰©', 'ç»†èƒ', 'é—ä¼ ', 'ç”Ÿæ€', 'æ¤ç‰©', 'åŠ¨ç‰©'],
            'å†å²': ['å†å²', 'æœä»£', 'äº‹ä»¶', 'äººç‰©'],
            'åœ°ç†': ['åœ°ç†', 'åœ°å½¢', 'æ°”å€™', 'å›½å®¶', 'åŸå¸‚']
        }
        
        detected_subjects = []
        query_lower = query.lower()
        
        for subject, keywords in subject_keywords.items():
            if any(keyword in query_lower for keyword in keywords):
                detected_subjects.append(subject)
        
        return detected_subjects
    
    def _calculate_knowledge_graph_relevance(self, kg, query: Optional[str], subject_name: str) -> float:
        """
        è®¡ç®—çŸ¥è¯†å›¾è°±çš„ç›¸å…³æ€§åˆ†æ•°
        """
        if not query:
            return 1.0
        
        relevance_score = 0.0
        query_lower = query.lower()
        
        # æå–æœç´¢å…³é”®è¯
        search_keywords = self._extract_search_keywords(query)
        
        # æ ‡é¢˜åŒ¹é…ï¼ˆæƒé‡æœ€é«˜ï¼‰
        title_matches = sum(1 for keyword in search_keywords if keyword in kg.name.lower())
        if title_matches > 0:
            relevance_score += 0.5 * (title_matches / len(search_keywords))
        
        # å†…å®¹åŒ¹é…
        if kg.content:
            content_matches = sum(1 for keyword in search_keywords if keyword in kg.content.lower())
            if content_matches > 0:
                relevance_score += 0.3 * (content_matches / len(search_keywords))
        
        # æè¿°åŒ¹é…
        if kg.description:
            desc_matches = sum(1 for keyword in search_keywords if keyword in kg.description.lower())
            if desc_matches > 0:
                relevance_score += 0.15 * (desc_matches / len(search_keywords))
        
        # æ ‡ç­¾åŒ¹é…
        if kg.tags:
            tag_matches = sum(1 for keyword in search_keywords 
                            for tag in kg.tags if keyword in tag.lower())
            if tag_matches > 0:
                relevance_score += 0.05 * min(tag_matches / len(search_keywords), 1.0)
        
        # å­¦ç§‘åŒ¹é…åŠ åˆ†
        subject_hints = self._extract_subject_hints(query)
        if subject_hints and subject_name in subject_hints:
            relevance_score += 0.1
        
        return min(relevance_score, 1.0)
    
    def _detect_question_subject(self, text: str) -> Optional[str]:
        """
        æ™ºèƒ½æ£€æµ‹é¢˜ç›®æˆ–é—®é¢˜çš„å­¦ç§‘
        """
        if not text:
            return None
            
        text_lower = text.lower()
        
        # å­¦ç§‘å…³é”®è¯æ˜ å°„
        subject_keywords = {
            'æ•°å­¦': ['æ•°å­¦', 'å‡½æ•°', 'æ–¹ç¨‹', 'å‡ ä½•', 'ä»£æ•°', 'å¾®ç§¯åˆ†', 'å¯¼æ•°', 'ç§¯åˆ†', 'ä¸‰è§’', 'æ¦‚ç‡', 'ç»Ÿè®¡', 'å‘é‡', 'çŸ©é˜µ', 'è§£é¢˜', 'è®¡ç®—', 'è¯æ˜'],
            'ç‰©ç†': ['ç‰©ç†', 'åŠ›å­¦', 'ç”µå­¦', 'å…‰å­¦', 'çƒ­å­¦', 'åŸå­', 'åˆ†å­', 'èƒ½é‡', 'åŠŸç‡', 'ç”µæµ', 'ç”µå‹', 'ç£åœº', 'æ³¢åŠ¨', 'æŒ¯åŠ¨', 'ç‰›é¡¿', 'æ¬§å§†'],
            'åŒ–å­¦': ['åŒ–å­¦', 'å…ƒç´ ', 'åŒ–åˆç‰©', 'ååº”', 'åˆ†å­å¼', 'åŸå­', 'ç¦»å­', 'é…¸ç¢±', 'æ°§åŒ–', 'è¿˜åŸ', 'æœ‰æœº', 'æ— æœº', 'å‚¬åŒ–', 'å¹³è¡¡'],
            'ç”Ÿç‰©': ['ç”Ÿç‰©', 'ç»†èƒ', 'åŸºå› ', 'è›‹ç™½è´¨', 'é…¶', 'DNA', 'RNA', 'é—ä¼ ', 'è¿›åŒ–', 'ç”Ÿæ€', 'æ¤ç‰©', 'åŠ¨ç‰©', 'å¾®ç”Ÿç‰©', 'æ–°é™ˆä»£è°¢'],
            'è¯­æ–‡': ['è¯­æ–‡', 'æ–‡è¨€æ–‡', 'å¤è¯—', 'ä½œæ–‡', 'é˜…è¯»ç†è§£', 'ä¿®è¾', 'è¯­æ³•', 'å­—è¯', 'æˆè¯­', 'è¯—æ­Œ', 'æ•£æ–‡', 'å°è¯´', 'è®®è®ºæ–‡'],
            'è‹±è¯­': ['è‹±è¯­', 'english', 'å•è¯', 'è¯­æ³•', 'é˜…è¯»', 'å†™ä½œ', 'å¬åŠ›', 'å£è¯­', 'ç¿»è¯‘', 'grammar', 'vocabulary', 'reading', 'writing'],
            'å†å²': ['å†å²', 'æœä»£', 'æˆ˜äº‰', 'æ”¿æ²»', 'ç»æµ', 'æ–‡åŒ–', 'ç¤¾ä¼š', 'æ”¹é©', 'é©å‘½', 'å¤ä»£', 'è¿‘ä»£', 'ç°ä»£', 'ä¸–ç•Œå²', 'ä¸­å›½å²'],
            'åœ°ç†': ['åœ°ç†', 'åœ°å½¢', 'æ°”å€™', 'äººå£', 'åŸå¸‚', 'å†œä¸š', 'å·¥ä¸š', 'äº¤é€š', 'ç¯å¢ƒ', 'èµ„æº', 'åœ°å›¾', 'ç»çº¬åº¦', 'æ¿å—', 'æ´‹æµ'],
            'æ”¿æ²»': ['æ”¿æ²»', 'é©¬å…‹æ€', 'å“²å­¦', 'ç»æµå­¦', 'æ³•å¾‹', 'é“å¾·', 'ç¤¾ä¼šä¸»ä¹‰', 'æ°‘ä¸»', 'æ³•æ²»', 'äººæƒ', 'å›½é™…å…³ç³»', 'æ”¿åºœ', 'åˆ¶åº¦']
        }
        
        # è®¡ç®—æ¯ä¸ªå­¦ç§‘çš„åŒ¹é…åº¦
        subject_scores = {}
        for subject, keywords in subject_keywords.items():
            score = sum(1 for keyword in keywords if keyword in text_lower)
            if score > 0:
                subject_scores[subject] = score
        
        # è¿”å›åŒ¹é…åº¦æœ€é«˜çš„å­¦ç§‘
        if subject_scores:
            return max(subject_scores.keys(), key=lambda x: subject_scores[x])
        
        return None
    
    def _detect_question_type(self, text: str) -> str:
        """
        æ™ºèƒ½æ£€æµ‹é¢˜ç›®ç±»å‹
        """
        if not text:
            return 'å…¶ä»–'
            
        text_lower = text.lower()
        
        # é¢˜ç›®ç±»å‹å…³é”®è¯
        if any(keyword in text_lower for keyword in ['é€‰æ‹©', 'å•é€‰', 'å¤šé€‰', 'abcd', 'ä¸‹åˆ—']):
            return 'é€‰æ‹©é¢˜'
        elif any(keyword in text_lower for keyword in ['å¡«ç©º', 'ç©ºæ ¼', '______']):
            return 'å¡«ç©ºé¢˜'
        elif any(keyword in text_lower for keyword in ['è®¡ç®—', 'æ±‚è§£', 'è§£ç­”', 'è¯æ˜']):
            return 'è§£ç­”é¢˜'
        elif any(keyword in text_lower for keyword in ['åˆ†æ', 'è®ºè¿°', 'ç®€ç­”', 'è¯´æ˜']):
            return 'åˆ†æé¢˜'
        elif any(keyword in text_lower for keyword in ['ä½œæ–‡', 'å†™ä½œ', 'è®®è®º', 'è®°å™']):
            return 'å†™ä½œé¢˜'
        elif any(keyword in text_lower for keyword in ['å®éªŒ', 'æ“ä½œ', 'è§‚å¯Ÿ']):
            return 'å®éªŒé¢˜'
        elif any(keyword in text_lower for keyword in ['ç¿»è¯‘', 'é˜…è¯»ç†è§£', 'å®Œå½¢å¡«ç©º']):
            return 'è¯­è¨€é¢˜'
        else:
            return 'ç»¼åˆé¢˜'
    
    def _get_learning_analytics_data(self, user_id: str) -> Dict[str, Any]:
        """
        è·å–å­¦ä¹ åˆ†ææ•°æ®
        
        Args:
            user_id: ç”¨æˆ·ID
        
        Returns:
            å­¦ä¹ åˆ†ææ•°æ®
        """
        try:
            from datetime import datetime, timedelta
            from sqlalchemy import func, case
            
            # è·å–æœ€è¿‘30å¤©çš„æ•°æ®
            thirty_days_ago = datetime.utcnow() - timedelta(days=30)
            
            # é”™é¢˜ç»Ÿè®¡
            mistake_stats = db.session.query(
                func.count(MistakeRecord.id).label('total_mistakes'),
                func.sum(case((MistakeRecord.is_resolved == True, 1), else_=0)).label('resolved_mistakes'),
                func.avg(MistakeRecord.mastery_level).label('avg_mastery')
            ).filter(
                MistakeRecord.user_id == user_id
            ).first()
            
            # è€ƒè¯•ç»Ÿè®¡
            exam_stats = db.session.query(
                func.count(ExamSession.id).label('total_exams'),
                func.avg(ExamSession.score_percentage).label('avg_score'),
                func.avg(ExamSession.time_efficiency).label('avg_efficiency')
            ).filter(
                ExamSession.user_id == user_id
            ).filter(
                ExamSession.created_time >= thirty_days_ago
            ).filter(
                ExamSession.status == 'completed' # type: ignore
            ).first()
            
            # å­¦ä¹ æ—¶é•¿ç»Ÿè®¡
            study_stats = db.session.query(
                func.sum(StudyRecord.duration).label('total_duration'),
                func.count(StudyRecord.id).label('total_sessions'),
                func.avg(StudyRecord.mastery_level).label('avg_mastery')
            ).filter(
                StudyRecord.user_id == user_id
            ).filter(
                StudyRecord.start_time >= thirty_days_ago
            ).first()
            
            return {
                'mistake_analysis': {
                    'total_mistakes': getattr(mistake_stats, 'total_mistakes', 0) or 0,
                    'resolved_mistakes': getattr(mistake_stats, 'resolved_mistakes', 0) or 0,
                    'average_mastery': float(getattr(mistake_stats, 'avg_mastery', 0) or 0),
                    'resolution_rate': (getattr(mistake_stats, 'resolved_mistakes', 0) or 0) / max(getattr(mistake_stats, 'total_mistakes', 1) or 1, 1) * 100
                },
                'exam_performance': {
                    'total_exams': getattr(exam_stats, 'total_exams', 0) or 0,
                    'average_score': float(getattr(exam_stats, 'avg_score', 0) or 0),
                    'average_efficiency': float(getattr(exam_stats, 'avg_efficiency', 0) or 0)
                },
                'study_habits': {
                    'total_duration': getattr(study_stats, 'total_duration', 0) or 0,
                    'total_sessions': getattr(study_stats, 'total_sessions', 0) or 0,
                    'average_mastery': float(getattr(study_stats, 'avg_mastery', 0) or 0),
                    'average_session_duration': (getattr(study_stats, 'total_duration', 0) or 0) / max(getattr(study_stats, 'total_sessions', 1) or 1, 1)
                }
            }
            
        except Exception as e:
            logger.error(f"è·å–å­¦ä¹ åˆ†ææ•°æ®å¤±è´¥: {str(e)}")
            return {}
    
    def _should_generate_ppt(self, message: str) -> bool:
        """
        æ£€æµ‹ç”¨æˆ·æ¶ˆæ¯æ˜¯å¦åŒ…å«ç”ŸæˆPPTçš„è¯·æ±‚
        """
        ppt_keywords = [
            'ppt', 'PPT', 'powerpoint', 'PowerPoint', 
            'å¹»ç¯ç‰‡', 'æ¼”ç¤ºæ–‡ç¨¿', 'è¯¾ä»¶', 'ç”Ÿæˆppt', 
            'åˆ¶ä½œppt', 'åˆ›å»ºppt', 'åšä¸ªppt', 'åšä¸€ä¸ªppt'
        ]
        
        message_lower = message.lower()
        return any(keyword.lower() in message_lower for keyword in ppt_keywords)
    
    def _handle_ppt_generation(self, user_id: str, message: str, template_id: Optional[str] = None) -> Dict[str, Any]:
        """
        å¤„ç†PPTç”Ÿæˆè¯·æ±‚
        """
        try:
            # è°ƒç”¨PPTç”ŸæˆæœåŠ¡
            result = ppt_generation_service.generate_ppt_from_text(
                user_id=user_id,
                tenant_id="default",
                content=message,
                title="AIç”Ÿæˆçš„æ¼”ç¤ºæ–‡ç¨¿",
                template_id=template_id
            )
            
            if result['success']:
                data = result['data']
                filename = data['title'] + '.pptx'
                return {
                    "success": True,
                    "response": f"æˆ‘å·²ç»ä¸ºæ‚¨ç”Ÿæˆäº†PPTï¼\n\nğŸ“„ æ–‡ä»¶å: {filename}\nğŸ”— ä¸‹è½½é“¾æ¥: {data['download_url']}\n\nè¯¥PPTå·²è‡ªåŠ¨ä¿å­˜åˆ°æ‚¨çš„æ–‡æ¡£ç®¡ç†ä¸­ï¼Œæ‚¨å¯ä»¥éšæ—¶æŸ¥çœ‹å’Œä¸‹è½½ã€‚",
                    "assistant_name": self.assistant_name,
                    "timestamp": datetime.now().isoformat(),
                    "ppt_info": {
                        "filename": filename,
                        "download_url": data['download_url'],
                        "document_id": data.get('document_id'),
                        "slides_count": data.get('slides_count')
                    },
                    "suggestions": [
                        "æ‚¨å¯ä»¥ç‚¹å‡»ä¸‹è½½é“¾æ¥è·å–PPTæ–‡ä»¶",
                        "PPTå·²ä¿å­˜åœ¨æ–‡æ¡£ç®¡ç†ä¸­ï¼Œå¯éšæ—¶æŸ¥çœ‹",
                        "å¦‚éœ€ä¿®æ”¹PPTå†…å®¹ï¼Œè¯·å‘Šè¯‰æˆ‘å…·ä½“è¦æ±‚"
                    ]
                }
            else:
                return {
                    "success": False,
                    "response": f"æŠ±æ­‰ï¼ŒPPTç”Ÿæˆå¤±è´¥ï¼š{result.get('error', 'æœªçŸ¥é”™è¯¯')}ã€‚è¯·ç¨åé‡è¯•æˆ–æä¾›æ›´è¯¦ç»†çš„å†…å®¹è¦æ±‚ã€‚",
                    "assistant_name": self.assistant_name,
                    "timestamp": datetime.now().isoformat()
                }
                
        except Exception as e:
            logger.error(f"PPTç”Ÿæˆå¤„ç†å¤±è´¥: {str(e)}")
            return {
                "success": False,
                "response": "æŠ±æ­‰ï¼ŒPPTç”ŸæˆåŠŸèƒ½æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•ã€‚",
                "error": str(e),
                "assistant_name": self.assistant_name,
                "timestamp": datetime.now().isoformat()
            }
    
    def generate_ppt(self, content: str, template_id: Optional[str] = None, 
                    user_id: str = "1", tenant_id: str = "default") -> Dict[str, Any]:
        """
        ç”ŸæˆPPT
        
        Args:
            content: PPTå†…å®¹æè¿°
            template_id: æ¨¡æ¿IDï¼ˆå¯é€‰ï¼‰
            user_id: ç”¨æˆ·ID
            tenant_id: ç§Ÿæˆ·ID
            
        Returns:
            Dict: åŒ…å«ç”Ÿæˆç»“æœçš„å­—å…¸
        """
        try:
            logger.info(f"å¼€å§‹ç”ŸæˆPPTï¼Œç”¨æˆ·ID: {user_id}, æ¨¡æ¿ID: {template_id}")
            
            # è°ƒç”¨PPTç”ŸæˆæœåŠ¡
            result = ppt_generation_service.generate_ppt_from_text(
                content=content,
                user_id=user_id,
                tenant_id=tenant_id,
                template_id=template_id
            )
            
            if result.get('success'):
                logger.info(f"PPTç”ŸæˆæˆåŠŸ: {result.get('filename')}")
                return {
                    "success": True,
                    "filename": result.get('filename'),
                    "download_url": result.get('download_url'),
                    "file_path": result.get('file_path'),
                    "message": "PPTç”ŸæˆæˆåŠŸ"
                }
            else:
                logger.error(f"PPTç”Ÿæˆå¤±è´¥: {result.get('error')}")
                return {
                    "success": False,
                    "message": result.get('error', 'PPTç”Ÿæˆå¤±è´¥'),
                    "error": result.get('error')
                }
                
        except Exception as e:
            logger.error(f"PPTç”Ÿæˆå¼‚å¸¸: {str(e)}")
            return {
                "success": False,
                "message": f"PPTç”Ÿæˆå¤±è´¥: {str(e)}",
                "error": str(e)
            }

# åˆ›å»ºå…¨å±€å®ä¾‹
ai_assistant_service = AIAssistantService()