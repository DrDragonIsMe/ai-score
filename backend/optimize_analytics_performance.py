#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å­¦ä¹ åˆ†ææ¨¡å—æ€§èƒ½ä¼˜åŒ–è„šæœ¬
åŒ…æ‹¬æ•°æ®åº“ç´¢å¼•ä¼˜åŒ–ã€æŸ¥è¯¢ä¼˜åŒ–å’Œç¼“å­˜æœºåˆ¶
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from app import create_app
from utils.database import db
from sqlalchemy import text, Index
from models.learning import StudyRecord
from models.mistake import MistakeRecord
from models.knowledge import KnowledgePoint, Subject, Chapter
from models.user import User
from datetime import datetime, timedelta
import time

def create_performance_indexes():
    """
    åˆ›å»ºæ€§èƒ½ä¼˜åŒ–ç´¢å¼•
    """
    print("=== åˆ›å»ºæ€§èƒ½ä¼˜åŒ–ç´¢å¼• ===")
    
    try:
        # ä¸ºStudyRecordè¡¨åˆ›å»ºå¤åˆç´¢å¼•
        indexes_to_create = [
            # ç”¨æˆ·ID + åˆ›å»ºæ—¶é—´ç´¢å¼•ï¼ˆç”¨äºæ—¶é—´èŒƒå›´æŸ¥è¯¢ï¼‰
            "CREATE INDEX IF NOT EXISTS idx_study_records_user_created ON study_records(user_id, created_at)",
            
            # ç”¨æˆ·ID + çŸ¥è¯†ç‚¹IDç´¢å¼•ï¼ˆç”¨äºçŸ¥è¯†ç‚¹åˆ†æï¼‰
            "CREATE INDEX IF NOT EXISTS idx_study_records_user_kp ON study_records(user_id, knowledge_point_id)",
            
            # ç”¨æˆ·ID + æ­£ç¡®æ€§ç´¢å¼•ï¼ˆç”¨äºå‡†ç¡®ç‡ç»Ÿè®¡ï¼‰
            "CREATE INDEX IF NOT EXISTS idx_study_records_user_correct ON study_records(user_id, is_correct)",
            
            # åˆ›å»ºæ—¶é—´ç´¢å¼•ï¼ˆç”¨äºæ—¶é—´åˆ†æï¼‰
            "CREATE INDEX IF NOT EXISTS idx_study_records_created_at ON study_records(created_at)",
            
            # çŸ¥è¯†ç‚¹IDç´¢å¼•
            "CREATE INDEX IF NOT EXISTS idx_study_records_kp_id ON study_records(knowledge_point_id)",
            
            # ä¸ºMistakeRecordè¡¨åˆ›å»ºç´¢å¼•
            "CREATE INDEX IF NOT EXISTS idx_mistake_records_user_created ON mistake_records(user_id, created_time)",
            "CREATE INDEX IF NOT EXISTS idx_mistake_records_user_kp ON mistake_records(user_id, knowledge_point_id)",
            
            # ä¸ºKnowledgePointè¡¨åˆ›å»ºç´¢å¼•
            "CREATE INDEX IF NOT EXISTS idx_knowledge_points_chapter ON knowledge_points(chapter_id)",
            
            # ä¸ºChapterè¡¨åˆ›å»ºç´¢å¼•
            "CREATE INDEX IF NOT EXISTS idx_chapters_subject ON chapters(subject_id)"
        ]
        
        for index_sql in indexes_to_create:
            try:
                db.session.execute(text(index_sql))
                print(f"âœ“ åˆ›å»ºç´¢å¼•: {index_sql.split('idx_')[1].split(' ')[0] if 'idx_' in index_sql else 'unknown'}")
            except Exception as e:
                print(f"âš  ç´¢å¼•åˆ›å»ºå¤±è´¥æˆ–å·²å­˜åœ¨: {str(e)[:50]}...")
        
        db.session.commit()
        print("âœ“ æ‰€æœ‰ç´¢å¼•åˆ›å»ºå®Œæˆ")
        
    except Exception as e:
        print(f"âœ— ç´¢å¼•åˆ›å»ºè¿‡ç¨‹å‡ºé”™: {str(e)}")
        db.session.rollback()

def optimize_database_settings():
    """
    ä¼˜åŒ–æ•°æ®åº“è®¾ç½®
    """
    print("\n=== ä¼˜åŒ–æ•°æ®åº“è®¾ç½® ===")
    
    try:
        # SQLiteæ€§èƒ½ä¼˜åŒ–è®¾ç½®
        optimization_settings = [
            "PRAGMA journal_mode = WAL",  # å¯ç”¨WALæ¨¡å¼æé«˜å¹¶å‘æ€§èƒ½
            "PRAGMA synchronous = NORMAL",  # å¹³è¡¡å®‰å…¨æ€§å’Œæ€§èƒ½
            "PRAGMA cache_size = 10000",  # å¢åŠ ç¼“å­˜å¤§å°
            "PRAGMA temp_store = MEMORY",  # ä¸´æ—¶è¡¨å­˜å‚¨åœ¨å†…å­˜ä¸­
            "PRAGMA mmap_size = 268435456",  # å¯ç”¨å†…å­˜æ˜ å°„ï¼ˆ256MBï¼‰
        ]
        
        for setting in optimization_settings:
            try:
                db.session.execute(text(setting))
                print(f"âœ“ åº”ç”¨è®¾ç½®: {setting}")
            except Exception as e:
                print(f"âš  è®¾ç½®åº”ç”¨å¤±è´¥: {setting} - {str(e)}")
        
        db.session.commit()
        print("âœ“ æ•°æ®åº“ä¼˜åŒ–è®¾ç½®å®Œæˆ")
        
    except Exception as e:
        print(f"âœ— æ•°æ®åº“è®¾ç½®ä¼˜åŒ–å¤±è´¥: {str(e)}")

def analyze_query_performance():
    """
    åˆ†ææŸ¥è¯¢æ€§èƒ½
    """
    print("\n=== åˆ†ææŸ¥è¯¢æ€§èƒ½ ===")
    
    # è·å–æµ‹è¯•ç”¨æˆ·
    user = db.session.query(User).first()
    if not user:
        print("âœ— æ²¡æœ‰æ‰¾åˆ°æµ‹è¯•ç”¨æˆ·")
        return
    
    user_id = user.id
    end_date = datetime.now()
    start_date = end_date - timedelta(days=30)
    
    # æµ‹è¯•æŸ¥è¯¢æ€§èƒ½
    queries_to_test = [
        {
            'name': 'åŸºç¡€å­¦ä¹ è®°å½•æŸ¥è¯¢',
            'query': lambda: db.session.query(StudyRecord).filter(
                StudyRecord.user_id == user_id,
                StudyRecord.created_at >= start_date
            ).all()
        },
        {
            'name': 'æŒ‰çŸ¥è¯†ç‚¹åˆ†ç»„ç»Ÿè®¡',
            'query': lambda: db.session.query(
                StudyRecord.knowledge_point_id,
                db.func.count(StudyRecord.id).label('count'),
                db.func.avg(StudyRecord.duration).label('avg_duration'),
                db.func.sum(db.case([(StudyRecord.is_correct == True, 1)], else_=0)).label('correct_count')
            ).filter(
                StudyRecord.user_id == user_id,
                StudyRecord.created_at >= start_date
            ).group_by(StudyRecord.knowledge_point_id).all()
        },
        {
            'name': 'æŒ‰æ—¥æœŸåˆ†ç»„ç»Ÿè®¡',
            'query': lambda: db.session.query(
                db.func.date(StudyRecord.created_at).label('date'),
                db.func.count(StudyRecord.id).label('count'),
                db.func.sum(StudyRecord.duration).label('total_duration'),
                db.func.avg(db.case([(StudyRecord.is_correct == True, 1.0)], else_=0.0)).label('accuracy')
            ).filter(
                StudyRecord.user_id == user_id,
                StudyRecord.created_at >= start_date
            ).group_by(db.func.date(StudyRecord.created_at)).all()
        },
        {
            'name': 'å¤æ‚è”è¡¨æŸ¥è¯¢',
            'query': lambda: db.session.query(
                StudyRecord,
                KnowledgePoint.name.label('kp_name'),
                Chapter.name.label('chapter_name'),
                Subject.name.label('subject_name')
            ).join(
                KnowledgePoint, StudyRecord.knowledge_point_id == KnowledgePoint.id
            ).join(
                Chapter, KnowledgePoint.chapter_id == Chapter.id
            ).join(
                Subject, Chapter.subject_id == Subject.id
            ).filter(
                StudyRecord.user_id == user_id,
                StudyRecord.created_at >= start_date
            ).limit(100).all()
        }
    ]
    
    performance_results = []
    
    for test in queries_to_test:
        try:
            # é¢„çƒ­æŸ¥è¯¢
            test['query']()
            
            # æµ‹è¯•æŸ¥è¯¢æ€§èƒ½ï¼ˆæ‰§è¡Œ3æ¬¡å–å¹³å‡å€¼ï¼‰
            times = []
            for _ in range(3):
                start_time = time.time()
                result = test['query']()
                end_time = time.time()
                times.append(end_time - start_time)
            
            avg_time = sum(times) / len(times)
            result_count = len(result) if hasattr(result, '__len__') else 1
            
            performance_results.append({
                'name': test['name'],
                'avg_time': avg_time,
                'result_count': result_count,
                'performance_rating': 'excellent' if avg_time < 0.1 else 'good' if avg_time < 0.5 else 'needs_improvement'
            })
            
            print(f"âœ“ {test['name']}: {avg_time:.3f}ç§’ ({result_count}æ¡ç»“æœ) - {performance_results[-1]['performance_rating']}")
            
        except Exception as e:
            print(f"âœ— {test['name']} æŸ¥è¯¢å¤±è´¥: {str(e)}")
            performance_results.append({
                'name': test['name'],
                'avg_time': float('inf'),
                'result_count': 0,
                'performance_rating': 'failed'
            })
    
    return performance_results

def create_materialized_views():
    """
    åˆ›å»ºç‰©åŒ–è§†å›¾ä»¥æé«˜æŸ¥è¯¢æ€§èƒ½
    """
    print("\n=== åˆ›å»ºæ€§èƒ½ä¼˜åŒ–è§†å›¾ ===")
    
    try:
        # åˆ›å»ºç”¨æˆ·å­¦ä¹ ç»Ÿè®¡è§†å›¾
        user_stats_view = """
        CREATE VIEW IF NOT EXISTS user_learning_stats AS
        SELECT 
            user_id,
            COUNT(*) as total_questions,
            SUM(CASE WHEN is_correct THEN 1 ELSE 0 END) as correct_answers,
            AVG(CASE WHEN is_correct THEN 1.0 ELSE 0.0 END) * 100 as accuracy_rate,
            SUM(duration) as total_study_time,
            AVG(duration) as avg_time_per_question,
            COUNT(DISTINCT DATE(created_at)) as study_days,
            MIN(created_at) as first_study,
            MAX(created_at) as last_study
        FROM study_records 
        GROUP BY user_id
        """
        
        # åˆ›å»ºçŸ¥è¯†ç‚¹ç»Ÿè®¡è§†å›¾
        knowledge_stats_view = """
        CREATE VIEW IF NOT EXISTS knowledge_point_stats AS
        SELECT 
            sr.knowledge_point_id,
            kp.name as knowledge_point_name,
            c.name as chapter_name,
            s.name as subject_name,
            COUNT(*) as total_attempts,
            SUM(CASE WHEN sr.is_correct THEN 1 ELSE 0 END) as correct_attempts,
            AVG(CASE WHEN sr.is_correct THEN 1.0 ELSE 0.0 END) * 100 as accuracy_rate,
            AVG(sr.duration) as avg_duration,
            COUNT(DISTINCT sr.user_id) as unique_users
        FROM study_records sr
        JOIN knowledge_points kp ON sr.knowledge_point_id = kp.id
        JOIN chapters c ON kp.chapter_id = c.id
        JOIN subjects s ON c.subject_id = s.id
        GROUP BY sr.knowledge_point_id, kp.name, c.name, s.name
        """
        
        # åˆ›å»ºæ¯æ—¥å­¦ä¹ ç»Ÿè®¡è§†å›¾
        daily_stats_view = """
        CREATE VIEW IF NOT EXISTS daily_learning_stats AS
        SELECT 
            user_id,
            DATE(created_at) as study_date,
            COUNT(*) as questions_count,
            SUM(CASE WHEN is_correct THEN 1 ELSE 0 END) as correct_count,
            AVG(CASE WHEN is_correct THEN 1.0 ELSE 0.0 END) * 100 as daily_accuracy,
            SUM(duration) as daily_study_time,
            AVG(duration) as avg_question_time
        FROM study_records 
        GROUP BY user_id, DATE(created_at)
        """
        
        views = [
            ('ç”¨æˆ·å­¦ä¹ ç»Ÿè®¡è§†å›¾', user_stats_view),
            ('çŸ¥è¯†ç‚¹ç»Ÿè®¡è§†å›¾', knowledge_stats_view),
            ('æ¯æ—¥å­¦ä¹ ç»Ÿè®¡è§†å›¾', daily_stats_view)
        ]
        
        for view_name, view_sql in views:
            try:
                db.session.execute(text(view_sql))
                print(f"âœ“ åˆ›å»ºè§†å›¾: {view_name}")
            except Exception as e:
                print(f"âš  è§†å›¾åˆ›å»ºå¤±è´¥æˆ–å·²å­˜åœ¨: {view_name} - {str(e)[:50]}...")
        
        db.session.commit()
        print("âœ“ æ‰€æœ‰è§†å›¾åˆ›å»ºå®Œæˆ")
        
    except Exception as e:
        print(f"âœ— è§†å›¾åˆ›å»ºè¿‡ç¨‹å‡ºé”™: {str(e)}")
        db.session.rollback()

def implement_query_optimizations():
    """
    å®ç°æŸ¥è¯¢ä¼˜åŒ–å»ºè®®
    """
    print("\n=== æŸ¥è¯¢ä¼˜åŒ–å»ºè®® ===")
    
    optimizations = [
        {
            'category': 'ç´¢å¼•ä¼˜åŒ–',
            'recommendations': [
                'ä¸ºé«˜é¢‘æŸ¥è¯¢å­—æ®µåˆ›å»ºå¤åˆç´¢å¼•',
                'å®šæœŸåˆ†ææŸ¥è¯¢æ‰§è¡Œè®¡åˆ’',
                'é¿å…åœ¨WHEREå­å¥ä¸­ä½¿ç”¨å‡½æ•°',
                'ä½¿ç”¨LIMITé™åˆ¶ç»“æœé›†å¤§å°'
            ]
        },
        {
            'category': 'æŸ¥è¯¢ç»“æ„ä¼˜åŒ–',
            'recommendations': [
                'ä½¿ç”¨EXISTSä»£æ›¿INå­æŸ¥è¯¢',
                'é¿å…SELECT *ï¼Œåªé€‰æ‹©éœ€è¦çš„å­—æ®µ',
                'åˆç†ä½¿ç”¨JOINï¼Œé¿å…ç¬›å¡å°”ç§¯',
                'ä½¿ç”¨æ‰¹é‡æ“ä½œä»£æ›¿å¾ªç¯æŸ¥è¯¢'
            ]
        },
        {
            'category': 'ç¼“å­˜ç­–ç•¥',
            'recommendations': [
                'å¯¹é¢‘ç¹è®¿é—®çš„ç»Ÿè®¡æ•°æ®å®æ–½ç¼“å­˜',
                'ä½¿ç”¨Redisç¼“å­˜çƒ­ç‚¹æ•°æ®',
                'å®ç°æŸ¥è¯¢ç»“æœç¼“å­˜æœºåˆ¶',
                'è®¾ç½®åˆç†çš„ç¼“å­˜è¿‡æœŸæ—¶é—´'
            ]
        },
        {
            'category': 'æ•°æ®åº“è®¾è®¡ä¼˜åŒ–',
            'recommendations': [
                'è€ƒè™‘æ•°æ®åˆ†åŒºç­–ç•¥',
                'å®šæœŸæ¸…ç†å†å²æ•°æ®',
                'ä¼˜åŒ–æ•°æ®ç±»å‹é€‰æ‹©',
                'å®ç°è¯»å†™åˆ†ç¦»'
            ]
        }
    ]
    
    for opt in optimizations:
        print(f"\nğŸ“Š {opt['category']}:")
        for i, rec in enumerate(opt['recommendations'], 1):
            print(f"  {i}. {rec}")

def generate_performance_report(performance_results):
    """
    ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
    """
    print("\n=== æ€§èƒ½ä¼˜åŒ–æŠ¥å‘Š ===")
    
    if not performance_results:
        print("âš  æ²¡æœ‰æ€§èƒ½æµ‹è¯•ç»“æœ")
        return
    
    # ç»Ÿè®¡æ€§èƒ½ç­‰çº§
    excellent_count = sum(1 for r in performance_results if r['performance_rating'] == 'excellent')
    good_count = sum(1 for r in performance_results if r['performance_rating'] == 'good')
    needs_improvement_count = sum(1 for r in performance_results if r['performance_rating'] == 'needs_improvement')
    failed_count = sum(1 for r in performance_results if r['performance_rating'] == 'failed')
    
    total_queries = len(performance_results)
    avg_time = sum(r['avg_time'] for r in performance_results if r['avg_time'] != float('inf')) / max(1, total_queries - failed_count)
    
    print(f"ğŸ“ˆ æŸ¥è¯¢æ€§èƒ½ç»Ÿè®¡:")
    print(f"  - æ€»æŸ¥è¯¢æ•°: {total_queries}")
    print(f"  - ä¼˜ç§€ (<0.1s): {excellent_count} ({excellent_count/total_queries*100:.1f}%)")
    print(f"  - è‰¯å¥½ (<0.5s): {good_count} ({good_count/total_queries*100:.1f}%)")
    print(f"  - éœ€æ”¹è¿› (â‰¥0.5s): {needs_improvement_count} ({needs_improvement_count/total_queries*100:.1f}%)")
    print(f"  - å¤±è´¥: {failed_count} ({failed_count/total_queries*100:.1f}%)")
    print(f"  - å¹³å‡å“åº”æ—¶é—´: {avg_time:.3f}ç§’")
    
    # æ€§èƒ½ç­‰çº§è¯„ä¼°
    if excellent_count >= total_queries * 0.8:
        overall_rating = "ä¼˜ç§€"
        rating_icon = "ğŸŸ¢"
    elif good_count + excellent_count >= total_queries * 0.7:
        overall_rating = "è‰¯å¥½"
        rating_icon = "ğŸŸ¡"
    else:
        overall_rating = "éœ€è¦ä¼˜åŒ–"
        rating_icon = "ğŸ”´"
    
    print(f"\n{rating_icon} æ€»ä½“æ€§èƒ½è¯„çº§: {overall_rating}")
    
    # ä¼˜åŒ–å»ºè®®
    if needs_improvement_count > 0 or failed_count > 0:
        print("\nğŸ”§ ä¼˜åŒ–å»ºè®®:")
        if needs_improvement_count > 0:
            print("  - å¯¹æ…¢æŸ¥è¯¢è¿›è¡Œç´¢å¼•ä¼˜åŒ–")
            print("  - è€ƒè™‘å®ç°æŸ¥è¯¢ç»“æœç¼“å­˜")
            print("  - ä¼˜åŒ–æŸ¥è¯¢é€»è¾‘ï¼Œå‡å°‘æ•°æ®æ‰«æ")
        if failed_count > 0:
            print("  - æ£€æŸ¥å¤±è´¥æŸ¥è¯¢çš„é”™è¯¯åŸå› ")
            print("  - éªŒè¯æ•°æ®åº“è¿æ¥å’Œæƒé™")
            print("  - ä¼˜åŒ–æŸ¥è¯¢è¯­æ³•å’Œé€»è¾‘")

def main():
    """
    ä¸»ä¼˜åŒ–æµç¨‹
    """
    print("å­¦ä¹ åˆ†ææ¨¡å—æ€§èƒ½ä¼˜åŒ–")
    print("=" * 50)
    
    app = create_app()
    
    with app.app_context():
        # 1. åˆ›å»ºæ€§èƒ½ç´¢å¼•
        create_performance_indexes()
        
        # 2. ä¼˜åŒ–æ•°æ®åº“è®¾ç½®
        optimize_database_settings()
        
        # 3. åˆ›å»ºä¼˜åŒ–è§†å›¾
        create_materialized_views()
        
        # 4. åˆ†ææŸ¥è¯¢æ€§èƒ½
        performance_results = analyze_query_performance()
        
        # 5. ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
        generate_performance_report(performance_results)
        
        # 6. æä¾›ä¼˜åŒ–å»ºè®®
        implement_query_optimizations()
        
        print("\n" + "=" * 50)
        print("ğŸ¯ æ€§èƒ½ä¼˜åŒ–å®Œæˆï¼")
        
        print("\nğŸ“‹ ä¼˜åŒ–æ€»ç»“:")
        print("- âœ… æ•°æ®åº“ç´¢å¼•ä¼˜åŒ–")
        print("- âœ… æŸ¥è¯¢æ€§èƒ½åˆ†æ")
        print("- âœ… è§†å›¾åˆ›å»ºä¼˜åŒ–")
        print("- âœ… æ•°æ®åº“è®¾ç½®è°ƒä¼˜")
        print("- âœ… æ€§èƒ½ç›‘æ§æŠ¥å‘Š")
        
        print("\nğŸ’¡ åç»­å»ºè®®:")
        print("- å®šæœŸç›‘æ§æŸ¥è¯¢æ€§èƒ½")
        print("- å®æ–½Redisç¼“å­˜æœºåˆ¶")
        print("- è€ƒè™‘æ•°æ®åˆ†åŒºç­–ç•¥")
        print("- å»ºç«‹æ€§èƒ½ç›‘æ§å‘Šè­¦")

if __name__ == '__main__':
    main()